{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fa8008-81f6-44cf-8474-efa95358cc5e",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "CS 510 Large Language Models - Winter 2024\n",
    "Bradley Thompson\n",
    "\n",
    "## 1 - Dataset Annotation\n",
    "\n",
    "To create the dataset, the authors utilized the Twitter API to retrieve 198 million tweets posted between May 2018 and March 2020. The original dataset included tweets in over thirty languages. It was filtered to consider only tweets with at least three tokens and without URLs to exclude bot tweets and spam advertising. Further, to create a balanced dataset for sentiment analysis, the authors made 8 distinct\n",
    "monolingual datasets. To ensure balance, a maximum number of tweets were established based on the size of the smallest dataset (3,033 tweets for Hindi) -- all other languages' datasets were pruned. The resulting dataset consists of 1,839 training tweets and 870 testing tweets, with a total size of 24,262 tweets. In terms of label distribution, the dataset was created with an equal distribution across the three labels, and the distribution is maintained throughout the train, test and validation sets.\n",
    "\n",
    "A few potential weaknesses of the annotation process are:\n",
    "- Intermixing languages with diverse scripts (only Arabic from a separate language family) could make it tough for the model to perform well on sentiment analysis tasks.\n",
    "- The labeling process itself is subjective, because an emotional interpretation of a given tweet's sentiment comes down to the annotator, and may not accurately track the tweet's actual sentiment. This is exacerbated by cultural differences between language speaking populations.\n",
    "- Aside from that, there isn't much information in the paper about the actual annotation process, specifically about the annotators themselves, so we can't know how qualified they were to analyze tweet sentiment.\n",
    "\n",
    "\n",
    "\n",
    "## 2 - Language Diversity\n",
    "\n",
    "| Language | Family | Resource Level (High / Low)|\n",
    "|----------|--------|----------------------------|\n",
    "|Arabic|Afro-Asiatic|Low|\n",
    "|English|Indo-European|High|\n",
    "|French|Indo-European|High|\n",
    "|German|Indo-European|High|\n",
    "|Hindi|Indo-European|Low|\n",
    "|Italian|Indo-European|Low|\n",
    "|Portoguese|Indo-European|Low|\n",
    "|Spanish|Indo-European|High\n",
    "\n",
    "Given this information, I believe that the high resource level languages will perform best, and I believe Arabic will perform significantly\n",
    "worse. This is a result of the overwhelming representation of data in the \"Indo-European\" family: Because these languages all will share common traits, it's possible that multi-language models will benefit from an understanding built on shared language patterns. Conversely, the only language (again, Arabic) representing an entirely different language family will garner no such benefit.\n",
    "\n",
    "Aside from these highlights, below I include the languages ranked from highest to lowest, as well as my thoughts on why I ranked each language as such inline:\n",
    "\n",
    "| Rank | Language | Notes |\n",
    "|------|----------|-------|\n",
    "|1|English|Obvious choice; Highest representation in data online|\n",
    "|2|Spanish|Close call with French/German, but wins because more Spanish speaking countries, so probably more representation|\n",
    "|3|French|Similar to German, but subjectively, seems like it could be more emotive, which will help with sentiment analysis|\n",
    "|4|German|Last slot of the high resource languages|\n",
    "|5|Hindi|Subjectively, I think this would be the highest representation low resource language|\n",
    "|6|Portoguese|Close behind Hindi, also because of assumed representation online|\n",
    "|7|Italian|I imagine that this is the lowest representation Indo-European language|\n",
    "|8|Arabic|Reasons referenced in preceding paragraph|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb063c4-33b6-47f7-8f78-75203bb633b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"When I\\'m soaring on Sunday afternoon, I learn Frank Gifford--one of my faves on the field and inside the broadcast booth--has died.\" '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "POSITIVE=\"positive\"\n",
    "NEUTRAL=\"neutral\"\n",
    "NEGATIVE=\"negative\"\n",
    "\n",
    "ID_TO_LABEL = {\n",
    "    0: NEGATIVE,\n",
    "    1: NEUTRAL,\n",
    "    2: POSITIVE,\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\")\n",
    "target_dataset = dataset[\"train\"]\n",
    "\n",
    "# Check out what the data looks like:\n",
    "\n",
    "positives = [ sample[\"text\"] for sample in target_dataset if ID_TO_LABEL[sample[\"label\"]] == POSITIVE ]\n",
    "neutral = [ sample[\"text\"] for sample in target_dataset if ID_TO_LABEL[sample[\"label\"]] == NEUTRAL ]\n",
    "negative = [ sample[\"text\"] for sample in target_dataset if ID_TO_LABEL[sample[\"label\"]] == NEGATIVE ]\n",
    "\n",
    "negative[189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687bdd99-afc9-4c3e-b08b-c5718104c7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
