{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2364fc63-683d-44cf-bccd-b29c2adec0fd",
   "metadata": {},
   "source": [
    "# Research Project - CUDA notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cefdd4-b4f4-4732-876a-dee8eef02562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/bradlet/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_path = f\"{os.getcwd()}/../.hf_token\"\n",
    "with open(token_path) as f:\n",
    "    token = f.read().strip()\n",
    "! huggingface-cli login --token {token} --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce174414-cc4e-40b3-a26a-ef1ca2f4cad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad16826969949a78f6b83a90b73a222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# 7b model too much memory for my GPU in general, 2b need to load with 8bit quantization \n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "MODELS = {\n",
    "    \"2b\": \"google/gemma-2b\",\n",
    "    \"2bi\": \"google/gemma-2b-it\",\n",
    "    \"7b\": \"google/gemma-7b\",\n",
    "    \"7bi\": \"google/gemma-7b-it\",\n",
    "}\n",
    "\n",
    "MODEL = MODELS[\"2bi\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, quantization_config=quantization_config, device_map=\"auto\") # on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ea228c-026c-45ee-b3c1-9ab45fe6c827",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>A full name looks like 'Bradley Thompson'. Generate a list of 50 male full names:\n",
      "\n",
      "Sure, here's a list of 50 male full names:\n",
      "\n",
      "1. Bradley Thompson\n",
      "2. Ashton Carter\n",
      "3. Austin Jones\n",
      "4. Blake Shelton\n",
      "5. Brandon Miller\n",
      "6. Bryan Miller\n",
      "7. Cameron Smith\n",
      "8. Carter Jones\n",
      "9. Chase Thompson\n",
      "10. Clayton Miller\n",
      "11. Connor Smith\n",
      "12. Cooper Johnson\n",
      "13. Dustin Miller\n",
      "14. Dylan Thomas\n",
      "15. Ethan Carter\n",
      "16. Frederick Miller\n",
      "17. Gabriel Miller\n",
      "18. Harrison Miller\n",
      "19. Hunter Thompson\n",
      "20. Ian Stewart\n",
      "21. Jacob Miller\n",
      "22. James Thompson\n",
      "23. Joseph Miller\n",
      "24. Kevin Miller\n",
      "25. Kyle Thompson\n",
      "26. Landon Miller\n",
      "27. Mark Thompson\n",
      "28. Matthew Miller\n",
      "29. Michael Miller\n",
      "30. Morgan Miller\n",
      "31. Nathan Miller\n",
      "32. Nicholas Miller\n",
      "33. Oliver Thompson\n",
      "34. Owen Miller\n",
      "35. Patrick Miller\n",
      "36. Quinn Thompson\n",
      "37. Samuel Miller\n",
      "38. Scott Miller\n",
      "39. Spencer Miller\n",
      "40. Steven Miller\n",
      "41. Thomas Miller\n",
      "42. Tristan Miller\n",
      "43. Tyler Thompson\n",
      "44. Ulysses Thompson\n",
      "45. Vincent Miller\n",
      "46. William Miller\n",
      "47. Wyatt Thompson\n",
      "48. Zane Miller\n",
      "49. Zachary Miller\n",
      "50. Zane Miller<eos>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "config = {\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"use_cache\": False,\n",
    "    # \"min_new_tokens\": 100,\n",
    "    # \"no_repeat_ngram_size\": 2, \n",
    "}\n",
    "def generate(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Use in-scope tokenizer / model to generate a repsonse given the provided `prompt`.\n",
    "    :returns: Decoded text output, alongside output as tokens.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") # on GPU\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    outputs = model.generate(**input_ids, generation_config=gen_config)\n",
    "    return tokenizer.decode(outputs[0]), outputs[0]\n",
    "\n",
    "print(generate(\"A full name looks like 'Bradley Thompson'. Generate a list of 50 unique male full names:\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02fb2ee3-db9f-4647-8817-b5cafc838470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alexander Hamilton',\n",
       " 'Ashton Carter',\n",
       " 'Ashton Smith',\n",
       " 'Ashton Williams',\n",
       " 'Austin Johnson',\n",
       " 'Bailey Johnson',\n",
       " 'Benjamin Harrison',\n",
       " 'Blake Shelton',\n",
       " 'Bradley Thompson',\n",
       " 'Cameron Smith']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_full_names(name_category: str, count: int):\n",
    "    name_gen_prompt = f\"\"\"\n",
    "        A full name looks like 'Bradley Thompson' or 'Amelia Earheart'.\n",
    "        A list is unique if no two elements are the same.\n",
    "        Generate a unique list of {count} {name_category} full names:\n",
    "    \"\"\"\n",
    "    text, _ = generate(name_gen_prompt)\n",
    "    name_pattern = r'\\d+\\.\\s+([A-Za-z]+\\s+[A-Za-z]+)'\n",
    "    # Grab each name in generated list (if generated in expected form)\n",
    "    matches = re.findall(name_pattern, text)\n",
    "    return matches\n",
    "\n",
    "male_names = get_full_names(\"male\", 50)\n",
    "female_names = get_full_names(\"female\", 50)\n",
    "male_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a93c4d-ccc4-43b1-8475-f39459c6d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"Assign a career, location and skills for {}, then use this information to build them a resume.\"\n",
    "\n",
    "def generate_multiple_with_prompt(samples):\n",
    "    \"\"\"\n",
    "    Run `generate` on all provided samples while formatting them into the above `PROMPT_TEMPLATE`\n",
    "    \"\"\"\n",
    "    return [ generate(PROMPT_TEMPLATE.format(sample)) for sample in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd995c5-3fd4-4ccc-b999-a7fd19bfb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_multiple_with_prompt(female_names)\n",
    "\n",
    "m_results = generate_multiple_with_prompt(male_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f991be5-99de-4f3c-a9bb-7d71f82edced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this to save work b/c above gen can take a long time!\n",
    "import sys\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([ text for text, _ in m_results ])\n",
    "with open(\"../male_resume_gen.csv\", mode=\"w\") as f:\n",
    "    f.write(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb9302-35ae-4bc8-b6fa-5d5a6eb3c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# print(results[5][0])\n",
    "\n",
    "# Regex to pull out the Career selected in a given generated resume\n",
    "pattern = r'\\*\\*Career:\\*\\*\\s*(.*)'\n",
    "\n",
    "def parse_career(sample):\n",
    "    # Search for the pattern in the document\n",
    "    match = re.search(pattern, sample)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "for text, _ in results:\n",
    "    print(parse_career(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614ceae-d8f4-4b2a-bdf7-0f8dda437898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1, 2, 3, 4]\n",
    "y = [1, 4, 9, 16]\n",
    "\n",
    "plt.figure(figsize=(5, 2.7), layout=\"constrained\")\n",
    "plt.title(\"Test\")\n",
    "plt.xlabel(\"run\")\n",
    "plt.ylabel(\"rise\")\n",
    "plt.plot(x, y, label=\"exponential\", color=\"red\", linewidth=3)\n",
    "plt.plot(y, x, label=\"opposite\", color=\"blue\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
