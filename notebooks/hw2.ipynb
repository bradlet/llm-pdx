{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2364fc63-683d-44cf-bccd-b29c2adec0fd",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Bradley Thompson - CS 510 Large Language Models PDX Winter 2024\n",
    "\n",
    "## Experimental Setting 1 - Preamble\n",
    "Note: Included a bunch of my long-form thinking as I iterated on the implementation to figure out how to get this classifier to work correctly.\n",
    "\n",
    "To start, I spent a long time just trying to figure out how to implement the classifier for this assignment. I got started right away, but blasted through many hours just trying to figure it out, not finding any really good resources online. I found one that threw me off base, because I started looking into using huggingface's provided AutoModelClassifiers. After talking to the professor, I learned that this was not the intention for the solution, so I switched back to basing my work on assignment 1. I didn't gain any significant headway until the TA linked some articles in Slack, which gave me a base to research on. I eventually got to the point that I realized how I could check the output scores for all tokens in the tokenizer's vocabulary, and then get the highest log probability for the tokens that mapped to my target labels, to get the models classification.\n",
    "\n",
    "At this point I ran into a few more residual issues around calculating metrics for the multi-class predictions, and also ran into an issue with the vocab because apparently bloom doesn't have \"Neutral\" in its embeddings. I wanted to make my labels case-insensitive with the hope that it would improve accuracy, so I went ahead and made a map to consider both component tokens (\"Neut\" and \"ral\") the same as \"neutral\".\n",
    "\n",
    "Finally, I was able to play around with hyper parameters. The above work quantifies almost all waking hours for a single weekend, and then many multi-hour sessions after work across 2 weeks. Luckily, I have a pretty nice GPU at home, and have CUDA support, so I am able to run the models pretty fast for no added Colab cost.\n",
    "\n",
    "I started with no fancy configuration other than setting the model to only generate 1 new token to start. My f1 score was around `0.16` with Bloom 560m on the training set. I set out with the goal of trying to maximize this to some extent, before running my classifier implementation for all models across the test set.\n",
    "\n",
    "First hyperparameter tweak was to try out `top_k` sampling; `top_k=2` had no effect on performance. Increasing to 4 saw an extremely marginal improvement, so I left it in before trying out `temperature` tuning. After decreasing temperature to `0.6` I noticed performance was unchanged. So I tried removing `top_k` and f1 score jumped up to `0.278`. So, for some reason, the combination of those sampling parameters was not good. I tested out two other generation configuration settings that I found as well: `epsilon_cutoff` and `eta_cutoff`. I didn't really look into what these do, just wanted to play around and see if there was any observable effect. First, `epsilon_cutoff` dropped my f1 score from `0.278` to around `0.2`; subsequently tested out `eta_cutoff`, with roughly the same result.\n",
    "\n",
    "After tweaking the above parameters, I decided that simply adjusting `temperature` and my input prompt would be sufficient for achieving decent results. Decreasing temperature from `0.6` to `0.3` had a negligible effect. As a final test to confirm what parameters I'd like to stick with before testing across all models, to compare and see the effect of model size on performance, I tried out a `temperature` value greater than the default. At a value of `1.5` my f1 score on the training set with Bloom 560m was roughly unchanged, which was surprising. It maybe suggested that the sampling method change, from greedy decoding, was what actually resulted in some improvement. So, using sampling was probably what caused the increase in f1 score. After realizing that, I tried out beam sampling to see if it had any effect. It also caused the performance to decrease. So, I stuck with sampling as my decoding strategy, vs. greedy search, and left the temperature at its default value.\n",
    "\n",
    "### Experimental Setting 1\n",
    "\n",
    "Finally, I let all models run on the test set and got these result metrics:\n",
    "\n",
    "```\n",
    "[\n",
    "    ('bigscience/bloom-560m', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloom-1b1', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloom-1b7', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloomz-560m', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloomz-1b1', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloomz-1b7', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805})\n",
    "]\n",
    "```\n",
    "\n",
    "So, clearly something was wrong with my implementation as the performance was unaffected by the model. At this point, I was starting to get close on time with the assignment deadline, though I'd spent a large sum of hours on the assignment already. Still, I tried to take a step back and see what was wrong.\n",
    "\n",
    "I retried the approach of focusing on the first model and checking the outputs. What I immediately noticed is that the log probs for the token was either wrong, or inhibiting the approach entirely. On a random sample I tried to classify and check the output probability for each token:\n",
    "\n",
    "```\n",
    "111017 - negative: 0.0\n",
    "149414 -  Negative: 0.0\n",
    "40979 -  neutral: 0.0\n",
    "76420 -  Neut: 0.0\n",
    "4343 - ral: 0.0\n",
    "18121 -  positive: 0.0\n",
    "139904 -  Positive: 0.0\n",
    "```\n",
    "\n",
    "So, clearly the model isn't going to classify well if it isn't assigning any value to my target tokens at all. In fact, I checked all output scores across the entire vocab, and the only tokens with values greater than zero were: `'</s>)\\n    a c b the ( 1 A T I \"\\n\\n J is - i it * In [ if The â€œ \\\\ It what This Is true false You If How There Do \"S What \\n\\n True \"The \"I Yes Your yes Does \"It'`. I was honestly surprised; I had assumed that all tokens in the vocab would have some likelihood that the model would consider, yet here I was seeing that the considered tokens were highly dependent on the input text (premise and hypothesis). My starting hypothesis was \"This text has a positive sentiment, true or false:\", which apparently resulted in none of the target label tokens being present in the model's guess for the next token. I started playing around with the hypothesis and found that this string was able to at least get all target label tokens identified and at least slightly considered for the next token: \"The text is neutral. Of the labels positive, negative or neutral, the text is:\". I had to double up the mention of \"neutral\" to even get it on the board!\n",
    "\n",
    "So, with this revelation, I looped back around to reconsidering my prompt a.k.a hypothesis, with the plan of retesting some parameters as well as comparing models. I settled on this string, which reiterated all my label options and then request one of the 3 target labels:\n",
    "\n",
    "```\n",
    "f\"Text categories: {LABELS}. Of the labels neutral, negative or positive, the text is:\"\n",
    "```\n",
    "\n",
    "This prompt was able always include at least some token that would represent one of each class. After viewing the output for a few samples, I decided to include more label tokens to additionally identify \"Pos\" and \"Neg\" as positive and negative, respectively.\n",
    "\n",
    "With that said, when I retried parameter tuning, as well as running on all models, and there was no effect on model performance -- all stayed the same. So in the end I just had to move on to the second experimental setting.\n",
    "\n",
    "### Experimental Setting 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce174414-cc4e-40b3-a26a-ef1ca2f4cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "MODELS = (\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"bigscience/bloomz-1b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    ")\n",
    "\n",
    "model_name = MODELS[0]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba1cd0c-c9ca-48e0-ad44-0fc713b4a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"Frank Gaffrey\\\\u002c Cliff May\\\\u002c Steve Emerson: Brilliant. \\\\\"\"\"\"Looming Threats: Iran\\\\u002c Hezbollah Hamas\\\\\"\"\"\" is the best #cufidc session I\\\\u2019ve had thus far.\" ',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\")\n",
    "target_dataset = dataset[\"train\"]\n",
    "\n",
    "# Check out what the data looks like:\n",
    "target_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e56a4f8-3cd6-456f-95bc-e0754d296423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering vocab indices for target tokens: negative Negative neutral Neutral positive Positive Pos Neg\n",
      "Token 111017: negative\n",
      "Token 149414:  Negative\n",
      "Token 40979:  neutral\n",
      "Token 76420:  Neut\n",
      "Token 4343: ral\n",
      "Token 18121:  positive\n",
      "Token 139904:  Positive\n",
      "Token 18683:  Pos\n",
      "Token 9775:  Neg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[111017, 149414, 40979, 76420, 4343, 18121, 139904, 18683, 9775]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "POSITIVE=\"positive\"\n",
    "NEUTRAL=\"neutral\"\n",
    "NEGATIVE=\"negative\"\n",
    "\n",
    "\n",
    "# Note: we have 2 options so we can be case insensitive\n",
    "LABELS = \"negative Negative neutral Neutral positive Positive Pos Neg\"\n",
    "ID_TO_LABEL = {\n",
    "    0: NEGATIVE,\n",
    "    1: NEUTRAL,\n",
    "    2: POSITIVE,\n",
    "}\n",
    "# Note: Found that bloom apparently doesn't have `Neutral` in its vocabulary; so, will consider either `Neut` or `ral`.\n",
    "TOKEN_INDEX_TO_LABEL_ID = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 2,\n",
    "    6: 2,\n",
    "    7: 2,\n",
    "    8: 0,\n",
    "}\n",
    "\n",
    "target_label_tokens = tokenizer(LABELS, add_special_tokens=False)[\"input_ids\"]\n",
    "print(\"Gathering vocab indices for target tokens:\", tokenizer.decode(target_label_tokens))\n",
    "for token in target_label_tokens:\n",
    "    print(f\"Token {token}: {tokenizer.decode(token)}\")\n",
    "target_label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ea228c-026c-45ee-b3c1-9ab45fe6c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: okay i\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\u2019mon America aren\\u2019t you sick of her yet? (sorry) \n",
      "Classification: negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "import torch as t\n",
    "\n",
    "config = {\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"max_new_tokens\": 1,\n",
    "    # \"do_sample\": True,\n",
    "    # \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "HYPOTHESIS = f\"Text categories: {LABELS}. Of the labels neutral, negative or positive, the text is:\"\n",
    "\n",
    "VERY_VERBOSE = False\n",
    "\n",
    "def classify(premise: str) -> int:\n",
    "    \"\"\"\n",
    "    Use model to generate output scores across entire vocab for the next token based on probability\n",
    "    of entailment for the given premise/hypothesis pair.\n",
    "    https://joeddav.github.io/blog/2020/05/29/ZSL.html#Classification-as-Natural-Language-Inference\n",
    "    :param premise: some input text string to be classified based on `LABELS`.\n",
    "    :returns: classification label id\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(premise, HYPOTHESIS, return_tensors=\"pt\")\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    # Get prediction scores across vocab for the only token generated b/c of gen config and normalize w/ softmax\n",
    "    output = model.generate(inputs, gen_config, return_dict_in_generate=True, output_scores=True)[\"scores\"][0]\n",
    "    vocab_probs = output.softmax(dim=1)\n",
    "    # Get probabilities of our target labels by index in our vocab\n",
    "    labels_log_probs = t.index_select(vocab_probs, 1, t.tensor(target_label_tokens))[0]\n",
    "    if VERY_VERBOSE:\n",
    "        for token, prob in zip(target_label_tokens, labels_log_probs):\n",
    "            print(f\"{token} - {tokenizer.decode(token)}: {prob}\")\n",
    "    # Get highest log prob label\n",
    "    labels_index = t.argmax(labels_log_probs).item()\n",
    "    if VERY_VERBOSE:\n",
    "        print(\"Selection index: \", labels_index)\n",
    "    return TOKEN_INDEX_TO_LABEL_ID[labels_index]\n",
    "\n",
    "sample = target_dataset[0][\"text\"]\n",
    "print(f\"Input text: {sample}\\nClassification: {ID_TO_LABEL[classify(sample)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "83a4cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\"\"\"\n",
    "Note: Uncomment the above line to run this cell\n",
    "This is used to test out what tokens are considered possible as the next token after premise/hypothesis.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "import torch as t\n",
    "\n",
    "config = {\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"do_sample\": True,\n",
    "    # \"temperature\": 1.5,\n",
    "}\n",
    "\n",
    "\n",
    "sample = target_dataset[0][\"text\"]\n",
    "HYPOTHESIS = f\"Text categories: {LABELS}. Of the labels neutral, negative or positive, the text is:\"\n",
    "\n",
    "inputs = tokenizer.encode(sample, HYPOTHESIS, return_tensors=\"pt\")\n",
    "gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "output = model.generate(inputs, gen_config, return_dict_in_generate=True, output_scores=True)[\"scores\"][0]\n",
    "\n",
    "mask = output[0] >= 0\n",
    "indices = mask.nonzero()\n",
    "tokenizer.decode(indices.transpose(0, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23759f0-d799-479b-b89a-b592a6bfb98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.4, 'precision': 0.6, 'f1': 0.4800000000000001}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "from typing import List\n",
    "\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def calculate_metrics(references: List[int], predictions: List[int]):\n",
    "    \"\"\"\n",
    "    Calculate recall, precision and f1 scores.\n",
    "    :returns: Dict of metric name to value across all predictions\n",
    "    \"\"\"\n",
    "    return recall_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys())) | \\\n",
    "        precision_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys())) | \\\n",
    "        f1_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys()))\n",
    "\n",
    "calculate_metrics([1, 1, 1, 2, 0], [1, 1, 0, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee87c7b3-9f7e-4380-8aaa-17c6c10343d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "\n",
    "def classify_with_step_reports(sample: str, model_name: str, step: int) -> int:\n",
    "    if VERBOSE:\n",
    "        print(f\"[{model_name}] classification step {step} / {len(target_dataset)}\")\n",
    "    return classify(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3608b0d5-578e-42d3-804c-1a12dca5e855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.32735182164219684,\n",
       " 'precision': 0.359562136553287,\n",
       " 'f1': 0.20588972452252285}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing out different hyper parameters on a single model (Bloom 560m) for the train set.\n",
    "references = target_dataset[\"label\"]\n",
    "predictions = [ classify_with_step_reports(sample, \"bloom 560m\", i) for i, sample in enumerate(target_dataset[\"text\"]) ]\n",
    "\n",
    "calculate_metrics(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34db79dc-8682-4428-877c-ab950d3259f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bigscience/bloom-560m',\n",
       "  {'recall': 0.3528735632183908,\n",
       "   'precision': 0.48830283101382843,\n",
       "   'f1': 0.24746735557369356}),\n",
       " ('bigscience/bloom-1b1',\n",
       "  {'recall': 0.3528735632183908,\n",
       "   'precision': 0.48830283101382843,\n",
       "   'f1': 0.24746735557369356}),\n",
       " ('bigscience/bloom-1b7',\n",
       "  {'recall': 0.3528735632183908,\n",
       "   'precision': 0.48830283101382843,\n",
       "   'f1': 0.24746735557369356}),\n",
       " ('bigscience/bloomz-560m',\n",
       "  {'recall': 0.3528735632183908,\n",
       "   'precision': 0.48830283101382843,\n",
       "   'f1': 0.24746735557369356}),\n",
       " ('bigscience/bloomz-1b1',\n",
       "  {'recall': 0.3528735632183908,\n",
       "   'precision': 0.48830283101382843,\n",
       "   'f1': 0.24746735557369356}),\n",
       " ('bigscience/bloomz-1b7',\n",
       "  {'recall': 0.3528735632183908,\n",
       "   'precision': 0.48830283101382843,\n",
       "   'f1': 0.24746735557369356})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run across all models \n",
    "from typing import Dict\n",
    "\n",
    "def run(model_name: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Get a tokenizer and model by model name, then classify all of `target_dataset` and calculate metrics.\n",
    "    :returns: Dict of calculated metrics for this model\n",
    "    \"\"\"\n",
    "    if VERBOSE:\n",
    "        print(f\"Starting classification on {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    target_dataset = dataset[\"test\"]\n",
    "    references = target_dataset[\"label\"]\n",
    "    predictions = [ classify_with_step_reports(sample, model_name, i) for i, sample in enumerate(target_dataset[\"text\"]) ]\n",
    "    return calculate_metrics(references, predictions)\n",
    "\n",
    "result_metrics = [ (model, run(model)) for model in MODELS ]\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543d78a-a524-4b13-ae13-33334729e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "def plot(all_metrics: List[Dict[str, float]]):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
