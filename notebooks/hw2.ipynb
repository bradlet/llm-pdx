{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2364fc63-683d-44cf-bccd-b29c2adec0fd",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Bradley Thompson - CS 510 Large Language Models PDX Winter 2024\n",
    "\n",
    "## Experimental Setting 1\n",
    "To start, I simply tried to set up my test bed for model performance. I missed the class where we disussed breaking down the model output\n",
    "by getting the log probability of the result tokens, and while it makes sense in theory, I don't know how to do it in practice. So, I instead\n",
    "decided to come up with a simple approach for checking on the generated output labels, by simply checking for substring presence and throwing out samples where a category can't be found. This is sub-par becuase it doesn't account for cases where multiple labels are selected in rambling output, and because it doesn't account for malformed output which might resemble a category option closely. I tried to remediate this at least slightly be limiting the number of new tokens generated, so that multiple labels were unlikely to be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce174414-cc4e-40b3-a26a-ef1ca2f4cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "MODELS = (\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"bigscience/bloomz-1b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    ")\n",
    "\n",
    "model_name = MODELS[0]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eba1cd0c-c9ca-48e0-ad44-0fc713b4a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"Frank Gaffrey\\\\u002c Cliff May\\\\u002c Steve Emerson: Brilliant. \\\\\"\"\"\"Looming Threats: Iran\\\\u002c Hezbollah Hamas\\\\\"\"\"\" is the best #cufidc session I\\\\u2019ve had thus far.\" ',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\")\n",
    "target_dataset = dataset[\"train\"]\n",
    "\n",
    "# Check out what the data looks like:\n",
    "target_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e56a4f8-3cd6-456f-95bc-e0754d296423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering vocab indices for target tokens: positive Positive negative Negative neutral Neutral\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[96675, 139904, 23381, 149414, 40979, 76420, 4343]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "POSITIVE=\"positive\"\n",
    "NEUTRAL=\"neutral\"\n",
    "NEGATIVE=\"negative\"\n",
    "\n",
    "\n",
    "# Note: we have 2 options so we can be case insensitive\n",
    "LABELS = \"negative Negative neutral Neutral positive Positive \"\n",
    "ID_TO_LABEL = {\n",
    "    0: NEGATIVE,\n",
    "    1: NEUTRAL,\n",
    "    2: POSITIVE,\n",
    "}\n",
    "\n",
    "def get_id_by_labels_index(labels_index: int) -> int:\n",
    "    \"\"\"\n",
    "    Find the target label id, same labels used in `tweet_sentiment_multilinqual` dataset.\n",
    "    For a label (index) `n`, `floor(n/2)` gives our target label id in `ID_TO_LABEL`.\n",
    "    \"\"\"\n",
    "    return math.floor(labels_index / 2)\n",
    "\n",
    "\n",
    "target_label_tokens = tokenizer(\"positive Positive negative Negative neutral Neutral\")[\"input_ids\"]\n",
    "print(\"Gathering vocab indices for target tokens:\", tokenizer.decode(target_label_tokens))\n",
    "target_label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38ea228c-026c-45ee-b3c1-9ab45fe6c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: okay i\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\u2019mon America aren\\u2019t you sick of her yet? (sorry) \n",
      "Classification: negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "import torch as t\n",
    "\n",
    "config = {\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"max_new_tokens\": 1,\n",
    "    # \"use_cache\": False,\n",
    "    # \"do_sample\": True,\n",
    "    # \"top_k\": 2,\n",
    "}\n",
    "\n",
    "HYPOTHESIS = \"This text has a positive sentiment, true or false:\"\n",
    "\n",
    "def classify(premise: str) -> int:\n",
    "    \"\"\"\n",
    "    Use model to generate output logits on target premise/hypothesis pair.\n",
    "    Result based on probability of entailment for the given hypothesis.\n",
    "    https://joeddav.github.io/blog/2020/05/29/ZSL.html#Classification-as-Natural-Language-Inference\n",
    "    :param premise: some input text string to be classified based on `LABELS`.\n",
    "    :returns: classification via index of target label in `LABELS`; use `get_labels_by_labels_index` for final label.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(premise, HYPOTHESIS, return_tensors=\"pt\")\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    # Get prediction scores across vocab for the only token generated b/c of gen config and normalize w/ softmax\n",
    "    output = model.generate(inputs, gen_config, return_dict_in_generate=True, output_scores=True)[\"scores\"][0]\n",
    "    vocab_probs = output.softmax(dim=1)\n",
    "    # Get probabilities of our target labels by index in our vocab\n",
    "    labels_log_probs = t.index_select(vocab_probs, 1, t.tensor(target_label_tokens))[0]\n",
    "    # Get highest log prob label\n",
    "    labels_index = t.argmax(labels_log_probs).item()\n",
    "    return get_id_by_labels_index(labels_index)\n",
    "\n",
    "sample = target_dataset[0][\"text\"]\n",
    "print(f\"Input text: {sample}\\nClassification: {ID_TO_LABEL[classify(sample)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a23759f0-d799-479b-b89a-b592a6bfb98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48239f96e0854683b38b288735d90248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.6666666666666666}\n",
      "{'precision': 1.0}\n",
      "{'f1': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "print(recall_metric.compute(references=[1, 1, 1, 0, 0], predictions=[1, 1, 0, 0, 0]))\n",
    "print(precision_metric.compute(references=[1, 1, 1, 0, 0], predictions=[1, 1, 0, 0, 0]))\n",
    "print(f1_metric.compute(references=[1, 1, 1, 0, 0], predictions=[1, 1, 0, 0, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
