{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2364fc63-683d-44cf-bccd-b29c2adec0fd",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Bradley Thompson - CS 510 Large Language Models PDX Winter 2024\n",
    "\n",
    "## Experimental Setting 1\n",
    "To start, I simply tried to set up my test bed for model performance. I missed the class where we disussed breaking down the model output\n",
    "by getting the log probability of the result tokens, and while it makes sense in theory, I don't know how to do it in practice. So, I instead\n",
    "decided to come up with a simple approach for checking on the generated output labels, by simply checking for substring presence and throwing out samples where a category can't be found. This is sub-par becuase it doesn't account for cases where multiple labels are selected in rambling output, and because it doesn't account for malformed output which might resemble a category option closely. I tried to remediate this at least slightly be limiting the number of new tokens generated, so that multiple labels were unlikely to be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce174414-cc4e-40b3-a26a-ef1ca2f4cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "MODELS = (\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"bigscience/bloomz-1b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    ")\n",
    "\n",
    "model_name = MODELS[0]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eba1cd0c-c9ca-48e0-ad44-0fc713b4a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"Frank Gaffrey\\\\u002c Cliff May\\\\u002c Steve Emerson: Brilliant. \\\\\"\"\"\"Looming Threats: Iran\\\\u002c Hezbollah Hamas\\\\\"\"\"\" is the best #cufidc session I\\\\u2019ve had thus far.\" ',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\")\n",
    "target_dataset = dataset[\"train\"]\n",
    "\n",
    "# Check out what the data looks like:\n",
    "target_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e56a4f8-3cd6-456f-95bc-e0754d296423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering vocab indices for target tokens: positive Positive negative Negative neutral Neutral\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[96675, 139904, 23381, 149414, 40979, 76420, 4343]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "POSITIVE=\"positive\"\n",
    "NEUTRAL=\"neutral\"\n",
    "NEGATIVE=\"negative\"\n",
    "\n",
    "\n",
    "# Note: we have 2 options so we can be case insensitive\n",
    "LABELS = \"negative Negative neutral Neutral positive Positive \"\n",
    "ID_TO_LABEL = {\n",
    "    0: NEGATIVE,\n",
    "    1: NEUTRAL,\n",
    "    2: POSITIVE,\n",
    "}\n",
    "\n",
    "def get_id_by_labels_index(labels_index: int) -> int:\n",
    "    \"\"\"\n",
    "    Find the target label id, same labels used in `tweet_sentiment_multilinqual` dataset.\n",
    "    For a label (index) `n`, `floor(n/2)` gives our target label id in `ID_TO_LABEL`.\n",
    "    \"\"\"\n",
    "    return math.floor(labels_index / 2)\n",
    "\n",
    "\n",
    "target_label_tokens = tokenizer(\"positive Positive negative Negative neutral Neutral\")[\"input_ids\"]\n",
    "print(\"Gathering vocab indices for target tokens:\", tokenizer.decode(target_label_tokens))\n",
    "target_label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38ea228c-026c-45ee-b3c1-9ab45fe6c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: okay i\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\u2019mon America aren\\u2019t you sick of her yet? (sorry) \n",
      "Classification: negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "import torch as t\n",
    "\n",
    "config = {\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"max_new_tokens\": 1,\n",
    "    # \"use_cache\": False,\n",
    "    # \"do_sample\": True,\n",
    "    # \"top_k\": 2,\n",
    "}\n",
    "\n",
    "HYPOTHESIS = \"This text has a positive sentiment, true or false:\"\n",
    "\n",
    "def classify(premise: str) -> int:\n",
    "    \"\"\"\n",
    "    Use model to generate output logits on target premise/hypothesis pair.\n",
    "    Result based on probability of entailment for the given hypothesis.\n",
    "    https://joeddav.github.io/blog/2020/05/29/ZSL.html#Classification-as-Natural-Language-Inference\n",
    "    :param premise: some input text string to be classified based on `LABELS`.\n",
    "    :returns: classification via index of target label in `LABELS`; use `get_labels_by_labels_index` for final label.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(premise, HYPOTHESIS, return_tensors=\"pt\")\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    # Get prediction scores across vocab for the only token generated b/c of gen config and normalize w/ softmax\n",
    "    output = model.generate(inputs, gen_config, return_dict_in_generate=True, output_scores=True)[\"scores\"][0]\n",
    "    vocab_probs = output.softmax(dim=1)\n",
    "    # Get probabilities of our target labels by index in our vocab\n",
    "    labels_log_probs = t.index_select(vocab_probs, 1, t.tensor(target_label_tokens))[0]\n",
    "    # Get highest log prob label\n",
    "    labels_index = t.argmax(labels_log_probs).item()\n",
    "    return get_id_by_labels_index(labels_index)\n",
    "\n",
    "sample = target_dataset[0][\"text\"]\n",
    "print(f\"Input text: {sample}\\nClassification: {ID_TO_LABEL[classify(sample)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a23759f0-d799-479b-b89a-b592a6bfb98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc88dcb52b94483ab9486b6975d7f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "To be able to use evaluate-metric/recall, you need to install the following dependencies['scikit-learn'] using 'pip install sklearn' for instance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m recall_metric \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m recall_metric\u001b[38;5;241m.\u001b[39mcompute(references\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], predictions\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/python/llm-pdx/env/lib/python3.11/site-packages/evaluate/loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/python/llm-pdx/env/lib/python3.11/site-packages/evaluate/loading.py:680\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (\u001b[38;5;167;01mConnectionError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m)):\n\u001b[0;32m--> 680\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    682\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hugging Face Hub either.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/python/llm-pdx/env/lib/python3.11/site-packages/evaluate/loading.py:639\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasurement\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubEvaluationModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluate-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 639\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/python/llm-pdx/env/lib/python3.11/site-packages/evaluate/loading.py:489\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    488\u001b[0m imports \u001b[38;5;241m=\u001b[39m get_imports(local_path)\n\u001b[0;32m--> 489\u001b[0m local_imports \u001b[38;5;241m=\u001b[39m \u001b[43m_download_additional_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_hub_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# copy the script and the files in an importable directory\u001b[39;00m\n\u001b[1;32m    496\u001b[0m dynamic_modules_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;28;01melse\u001b[39;00m init_dynamic_modules()\n",
      "File \u001b[0;32m~/Projects/python/llm-pdx/env/lib/python3.11/site-packages/evaluate/loading.py:265\u001b[0m, in \u001b[0;36m_download_additional_modules\u001b[0;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[1;32m    263\u001b[0m         needs_to_be_installed\u001b[38;5;241m.\u001b[39madd((library_import_name, library_import_path))\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_to_be_installed:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be able to use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, you need to install the following dependencies\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[lib_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlib_name,\u001b[38;5;250m \u001b[39mlib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mneeds_to_be_installed]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlib_name,\u001b[38;5;250m \u001b[39mlib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mneeds_to_be_installed])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for instance\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_imports\n",
      "\u001b[0;31mImportError\u001b[0m: To be able to use evaluate-metric/recall, you need to install the following dependencies['scikit-learn'] using 'pip install sklearn' for instance'"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "recall_metric.compute(references=[0, 1], predictions=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f76fb-41c2-4512-94f8-651d8eb13f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on set and evaluate:\n",
    "\n",
    "total_count = len(target_dataset)\n",
    "p = 0\n",
    "tp = 0\n",
    "neg = 0\n",
    "tneg = 0\n",
    "neut = 0\n",
    "tneut = 0\n",
    "for i, sample in enumerate(target_dataset):\n",
    "    if i == 5: # TODO REMOVE\n",
    "        break\n",
    "    input = sample[\"text\"]\n",
    "    label = sample[\"label\"]\n",
    "    print(LABEL_TO_ID[label])\n",
    "    output = output_raw_classification(DEFAULT_PROMPT % input).lower()\n",
    "    print(output)\n",
    "    if POSITIVE in output:\n",
    "        p += 1\n",
    "        if LABEL_TO_ID[label] == POSITIVE:\n",
    "            tp += 1\n",
    "    elif NEGATIVE in output:\n",
    "        neg += 1\n",
    "        if LABEL_TO_ID[label] == NEGATIVE:\n",
    "            tneg += 1\n",
    "    elif NEUTRAL in output:\n",
    "        neut += 1\n",
    "        if LABEL_TO_ID[label] == NEUTRAL:\n",
    "            tneut += 1\n",
    "    print(f\"Step {i}: categorized {p + neg + neut} / {total_count} | pos {p} ; neg {neg} ; neut {neut}\")\n",
    "fp = p - tp\n",
    "fneg = neg - tneg\n",
    "fneut = neut - tneut\n",
    "p_precision = tp / p\n",
    "p_recall = tp / len([ label for label in target_dataset[:5][\"label\"] if LABEL_TO_ID[label] == POSITIVE ])\n",
    "print(f\"pos prec {p_precision} pos rec {p_recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
