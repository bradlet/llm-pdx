{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2364fc63-683d-44cf-bccd-b29c2adec0fd",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Bradley Thompson - CS 510 Large Language Models PDX Winter 2024\n",
    "\n",
    "## Experimental Setting 1 - Preamble\n",
    "TL;DR: I struggled really hard to get to a working implementation for this assignment. It took a really long time to figure out before I could even run the experiments. Skip the \"Preamble\" to avoid my long-form thought process as I worked through making the classifier work, over a period of weeks.\n",
    "\n",
    "To start, I spent a long time just trying to figure out how to implement the classifier for this assignment. I got started right away, but blasted through many hours just trying to figure it out, not finding any really good resources online. I found one that threw me off base, because I started looking into using huggingface's provided AutoModelClassifiers. After talking to the professor, I learned that this was not the intention for the solution, so I switched back to basing my work on assignment 1. I didn't gain any significant headway until the TA linked some articles in Slack, which gave me a base to research on. I eventually got to the point that I realized how I could check the output scores for all tokens in the tokenizer's vocabulary, and then get the highest log probability for the tokens that mapped to my target labels, to get the models classification.\n",
    "\n",
    "At this point I ran into a few more residual issues around calculating metrics for the multi-class predictions, and also ran into an issue with the vocab because apparently bloom doesn't have \"Neutral\" in its embeddings. I wanted to make my labels case-insensitive with the hope that it would improve accuracy, so I went ahead and made a map to consider both component tokens (\"Neut\" and \"ral\") the same as \"neutral\".\n",
    "\n",
    "Finally, I was able to play around with hyper parameters. The above work quantifies almost all waking hours for a single weekend, and then many multi-hour sessions after work across 2 weeks. Luckily, I have a pretty nice GPU at home, and have CUDA support, so I am able to run the models pretty fast for no added Colab cost.\n",
    "\n",
    "I started with no fancy configuration other than setting the model to only generate 1 new token to start. My f1 score was around `0.16` with Bloom 560m on the training set. I set out with the goal of trying to maximize this to some extent, before running my classifier implementation for all models across the test set.\n",
    "\n",
    "First hyperparameter tweak was to try out `top_k` sampling; `top_k=2` had no effect on performance. Increasing to 4 saw an extremely marginal improvement, so I left it in before trying out `temperature` tuning. After decreasing temperature to `0.6` I noticed performance was unchanged. So I tried removing `top_k` and f1 score jumped up to `0.278`. So, for some reason, the combination of those sampling parameters was not good. I tested out two other generation configuration settings that I found as well: `epsilon_cutoff` and `eta_cutoff`. I didn't really look into what these do, just wanted to play around and see if there was any observable effect. First, `epsilon_cutoff` dropped my f1 score from `0.278` to around `0.2`; subsequently tested out `eta_cutoff`, with roughly the same result.\n",
    "\n",
    "After tweaking the above parameters, I decided that simply adjusting `temperature` and my input prompt would be sufficient for achieving decent results. Decreasing temperature from `0.6` to `0.3` had a negligible effect. As a final test to confirm what parameters I'd like to stick with before testing across all models, to compare and see the effect of model size on performance, I tried out a `temperature` value greater than the default. At a value of `1.5` my f1 score on the training set with Bloom 560m was roughly unchanged, which was surprising. It maybe suggested that the sampling method change, from greedy decoding, was what actually resulted in some improvement. So, using sampling was probably what caused the increase in f1 score. After realizing that, I tried out beam sampling to see if it had any effect. It also caused the performance to decrease. So, I stuck with sampling as my decoding strategy, vs. greedy search, and left the temperature at its default value.\n",
    "\n",
    "Finally, I let all models run on the test set and got these result metrics:\n",
    "\n",
    "```\n",
    "[\n",
    "    ('bigscience/bloom-560m', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloom-1b1', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloom-1b7', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloomz-560m', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloomz-1b1', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805}),\n",
    "    ('bigscience/bloomz-1b7', {'recall': 0.406896551724138, 'precision': 0.2733517463851896, 'f1': 0.32601205857019805})\n",
    "]\n",
    "```\n",
    "\n",
    "So, clearly something was wrong with my implementation as the performance was unaffected by the model. At this point, I was starting to get close on time with the assignment deadline, though I'd spent a large sum of hours on the assignment already. Still, I tried to take a step back and see what was wrong.\n",
    "\n",
    "I retried the approach of focusing on the first model and checking the outputs. What I immediately noticed is that the log probs for the token was either wrong, or inhibiting the approach entirely. On a random sample I tried to classify and check the output probability for each token:\n",
    "\n",
    "```\n",
    "111017 - negative: 0.0\n",
    "149414 -  Negative: 0.0\n",
    "40979 -  neutral: 0.0\n",
    "76420 -  Neut: 0.0\n",
    "4343 - ral: 0.0\n",
    "18121 -  positive: 0.0\n",
    "139904 -  Positive: 0.0\n",
    "```\n",
    "\n",
    "So, clearly the model isn't going to classify well if it isn't assigning any value to my target tokens at all. In fact, I checked all output scores across the entire vocab, and the only tokens with values greater than zero were: `'</s>)\\n    a c b the ( 1 A T I \"\\n\\n J is - i it * In [ if The â€œ \\\\ It what This Is true false You If How There Do \"S What \\n\\n True \"The \"I Yes Your yes Does \"It'`. I was honestly surprised; I had assumed that all tokens in the vocab would have some likelihood that the model would consider, yet here I was seeing that the considered tokens were highly dependent on the input text (premise and hypothesis). My starting hypothesis was \"This text has a positive sentiment, true or false:\", which apparently resulted in none of the target label tokens being present in the model's guess for the next token. I started playing around with the hypothesis and found that this string was able to at least get all target label tokens identified and at least slightly considered for the next token: \"The text is neutral. Of the labels positive, negative or neutral, the text is:\". I had to double up the mention of \"neutral\" to even get it on the board!\n",
    "\n",
    "So, with this revelation, I looped back around to reconsidering my prompt a.k.a hypothesis, with the plan of retesting some parameters as well as comparing models. I settled on this string, which reiterated all my label options and then request one of the 3 target labels:\n",
    "\n",
    "```\n",
    "f\"Text categories: {LABELS}. Of the labels neutral, negative or positive, the text is:\"\n",
    "```\n",
    "\n",
    "This prompt was able always include at least some token that would represent one of each class. After viewing the output for a few samples, I decided to include more label tokens to additionally identify \"Pos\" and \"Neg\" as positive and negative, respectively. Around this time, I also discovered the bug that caused all of the models to seemingly perform the same. With that I was finally able to actually run the experiments.\n",
    "\n",
    "### Experimental Setting 1\n",
    "\n",
    "After struggling through, as mentioned above in the \"Preamble\", I settled on not using any special generation configuration other than to force the model to only generate the output scores across the vocab for the next token. Using the hypothesis shown above, at the end of the \"Preamble\", my results across all models were as shown:\n",
    "\n",
    "```\n",
    "[\n",
    "    ('bigscience/bloom-560m', {'recall': 0.3528735632183908, 'precision': 0.48830283101382843, 'f1': 0.24746735557369356}),\n",
    "    ('bigscience/bloom-1b1', {'recall': 0.3367816091954023, 'precision': 0.33994349198810164, 'f1': 0.33502215074772324}),\n",
    "    ('bigscience/bloom-1b7', {'recall': 0.32298850574712645, 'precision': 0.3776534341585666, 'f1': 0.29099480881403705}),\n",
    "    ('bigscience/bloomz-560m', {'recall': 0.3620689655172414, 'precision': 0.423311805280963, 'f1': 0.22321465860714276}),\n",
    "    ('bigscience/bloomz-1b1', {'recall': 0.5091954022988506, 'precision': 0.351320632066126, 'f1': 0.40575125393382955}),\n",
    "    ('bigscience/bloomz-1b7', {'recall': 0.5287356321839081, 'precision': 0.3728617047408499, 'f1': 0.42572336676382916})\n",
    "]\n",
    "```\n",
    "\n",
    "So, I can finally tell that the 1 billion+ parameter models tended to perform much better, and the top performer was `bloomz-1b7` with a zero-shot f1 score of `0.42` on the test set. Given the implementation, it makes sense that having more parameters would help the model map the output scores more accurately for the next token. Another thing to note is that in general the `bloomz` models seem to perform better.\n",
    "\n",
    "### Experimental Setting 2\n",
    "\n",
    "For experimental setting 2, I simply prepended the hypothesis from setting 1 with 4 examples from the training set:\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "And the results across the test set were as follows:\n",
    "\n",
    "```\n",
    "[\n",
    "    ('bigscience/bloom-560m', {'recall': 0.3333333333333333, 'precision': 0.1111111111111111, 'f1': 0.16666666666666666}),\n",
    "    ('bigscience/bloom-1b1', {'recall': 0.3333333333333333, 'precision': 0.1111111111111111, 'f1': 0.16666666666666666}),\n",
    "    ('bigscience/bloom-1b7', {'recall': 0.34942528735632183, 'precision': 0.23391110202133825, 'f1': 0.2677628446522097}),\n",
    "    ('bigscience/bloomz-560m', {'recall': 0.36091954022988504, 'precision': 0.4005203031331297, 'f1': 0.22110362913083095}),\n",
    "    ('bigscience/bloomz-1b1', {'recall': 0.4517241379310345, 'precision': 0.33381423918508313, 'f1': 0.35386716943845653}),\n",
    "    ('bigscience/bloomz-1b7', {'recall': 0.3816091954022989, 'precision': 0.3732510288065844, 'f1': 0.260952380952381})\n",
    "]\n",
    "```\n",
    "\n",
    "So there was a noticeable decline in quality with the additional text. I figure the issue lies in the fact that the additional text doesn't help the model understand that the next token has to be the category label. I feel like the core issue might lie in my classifier implementation itself; thinking through it, really the thing that these experiments demonstrated is the fact that more parameters helps the models form better embeddings. It doesn't seem like the prompt has much effect other than changing the general set of tokens that are considered possible in the next step at all. Perhaps it would be better to allow the model to generate more than one token, so that we could effectively filter out all the cases where the model really wants to put a token like some piece of punctuation, so the target label tokens all have a low value.\n",
    "\n",
    "For example, if the input tweet text is \"Life is great\", the model could highly value many different punctuation tokens to mark the end of that clause: '.'; ','; ';', ':'. Anecdotally, I checked out the tokens (see the only disabled cell below) when I was trying to fix some core issues with my output score generation (see \"Preamble\" for more details), and strange tokens like those mentioned and new line characters were normally amongst the highest choices. Perhaps if these tokens weren't introducing as much noise, by searching more tokens that are generated, the models processing of previous text in the prompt would have a greater effect.\n",
    "\n",
    "Unfortunately, because of all the time lost (see \"Preamble\") I had officially ran out of time to iterate further after completing my 4-shot attempt for experimental setting 2. In the future, to continue to try and improve the classifer's performance for experimental setting 2, I would try to look into generating more tokens, then grabbing the average highest log prob token from our label set across all of the generated tokens. Perhaps this would also make the hyperparameter tuning more important, as I didn't notice much effect (discussion, again, primarily in the \"Preamble\").\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce174414-cc4e-40b3-a26a-ef1ca2f4cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "MODELS = (\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"bigscience/bloomz-1b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    ")\n",
    "\n",
    "model_name = MODELS[0]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eba1cd0c-c9ca-48e0-ad44-0fc713b4a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"Frank Gaffrey\\\\u002c Cliff May\\\\u002c Steve Emerson: Brilliant. \\\\\"\"\"\"Looming Threats: Iran\\\\u002c Hezbollah Hamas\\\\\"\"\"\" is the best #cufidc session I\\\\u2019ve had thus far.\" ',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\")\n",
    "target_dataset = dataset[\"train\"]\n",
    "\n",
    "# Check out what the data looks like:\n",
    "target_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e56a4f8-3cd6-456f-95bc-e0754d296423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering vocab indices for target tokens: negative Negative neutral Neutral positive Positive Pos Neg\n",
      "Token 111017: negative\n",
      "Token 149414:  Negative\n",
      "Token 40979:  neutral\n",
      "Token 76420:  Neut\n",
      "Token 4343: ral\n",
      "Token 18121:  positive\n",
      "Token 139904:  Positive\n",
      "Token 18683:  Pos\n",
      "Token 9775:  Neg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[111017, 149414, 40979, 76420, 4343, 18121, 139904, 18683, 9775]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "POSITIVE=\"positive\"\n",
    "NEUTRAL=\"neutral\"\n",
    "NEGATIVE=\"negative\"\n",
    "\n",
    "\n",
    "# Note: we have 2 options so we can be case insensitive\n",
    "LABELS = \"negative Negative neutral Neutral positive Positive Pos Neg\"\n",
    "ID_TO_LABEL = {\n",
    "    0: NEGATIVE,\n",
    "    1: NEUTRAL,\n",
    "    2: POSITIVE,\n",
    "}\n",
    "# Note: Found that bloom apparently doesn't have `Neutral` in its vocabulary; so, will consider either `Neut` or `ral`.\n",
    "TOKEN_INDEX_TO_LABEL_ID = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 2,\n",
    "    6: 2,\n",
    "    7: 2,\n",
    "    8: 0,\n",
    "}\n",
    "\n",
    "target_label_tokens = tokenizer(LABELS, add_special_tokens=False)[\"input_ids\"]\n",
    "print(\"Gathering vocab indices for target tokens:\", tokenizer.decode(target_label_tokens))\n",
    "for token in target_label_tokens:\n",
    "    print(f\"Token {token}: {tokenizer.decode(token)}\")\n",
    "target_label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04f955b4-c953-44c4-8e5d-fa425e491165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text: okay i\\\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\\\u2019mon America aren\\\\u2019t you sick of her yet? (sorry) , Label: negative; Text: @user the DC comics site has Batman 44 releases on the 9th but its out now? , Label: neutral; Text: \"Frank Gaffrey\\\\u002c Cliff May\\\\u002c Steve Emerson: Brilliant. \\\\\"\"\"\"Looming Threats: Iran\\\\u002c Hezbollah Hamas\\\\\"\"\"\" is the best #cufidc session I\\\\u2019ve had thus far.\" , Label: positive; Text: The tragedy of only thinking up hilarious tweets for the Summer Olympics now is that in four years there may be no place for them. , Label: negative; '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = dataset[\"train\"]\n",
    "shot_count = 4\n",
    "samples = [ f\"Text: {text}, Label: {ID_TO_LABEL[label]}; \" for text, label in zip(test_set[\"text\"][:shot_count], test_set[\"label\"][:shot_count]) ]\n",
    "hypothesis_prefix = \"\".join(samples)\n",
    "hypothesis_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38ea228c-026c-45ee-b3c1-9ab45fe6c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: okay i\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\u2019mon America aren\\u2019t you sick of her yet? (sorry) \n",
      "Classification: negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "import torch as t\n",
    "\n",
    "config = {\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"max_new_tokens\": 1,\n",
    "    # \"do_sample\": True,\n",
    "    # \"temperature\": 0.3,\n",
    "}\n",
    "\n",
    "HYPOTHESIS = f\"Text categories: {LABELS}. Of the labels neutral, negative or positive, the text label is:\"\n",
    "# For zero-shot, comment this out\n",
    "HYPOTHESIS = f\"These examples show some text and then a category label. {hypothesis_prefix}{HYPOTHESIS}\"\n",
    "\n",
    "VERY_VERBOSE = False\n",
    "\n",
    "def classify(tokenizer: AutoTokenizer, model: AutoModelForCausalLM, premise: str) -> int:\n",
    "    \"\"\"\n",
    "    Use model to generate output scores across entire vocab for the next token based on probability\n",
    "    of entailment for the given premise/hypothesis pair.\n",
    "    https://joeddav.github.io/blog/2020/05/29/ZSL.html#Classification-as-Natural-Language-Inference\n",
    "    :param premise: some input text string to be classified based on `LABELS`.\n",
    "    :returns: classification label id\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(premise, HYPOTHESIS, return_tensors=\"pt\")\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    # Get prediction scores across vocab for the only token generated b/c of gen config and normalize w/ softmax\n",
    "    output = model.generate(inputs, gen_config, return_dict_in_generate=True, output_scores=True)[\"scores\"][0]\n",
    "    vocab_probs = output.softmax(dim=1)\n",
    "    # Get probabilities of our target labels by index in our vocab\n",
    "    labels_log_probs = t.index_select(vocab_probs, 1, t.tensor(target_label_tokens))[0]\n",
    "    if VERY_VERBOSE:\n",
    "        for token, prob in zip(target_label_tokens, labels_log_probs):\n",
    "            print(f\"{token} - {tokenizer.decode(token)}: {prob}\")\n",
    "    # Get highest log prob label\n",
    "    labels_index = t.argmax(labels_log_probs).item()\n",
    "    if VERY_VERBOSE:\n",
    "        print(\"Selection index: \", labels_index)\n",
    "    return TOKEN_INDEX_TO_LABEL_ID[labels_index]\n",
    "\n",
    "sample = target_dataset[0][\"text\"]\n",
    "print(f\"Input text: {sample}\\nClassification: {ID_TO_LABEL[classify(tokenizer, model, sample)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edc60104",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\"\"\"\n",
    "Note: Uncomment the above line to run this cell\n",
    "This is used to test out what tokens are considered possible as the next token after premise/hypothesis.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "import torch as t\n",
    "\n",
    "config = {\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"do_sample\": True,\n",
    "    # \"temperature\": 1.5,\n",
    "}\n",
    "\n",
    "\n",
    "sample = target_dataset[0][\"text\"]\n",
    "HYPOTHESIS = f\"Text categories: {LABELS}. Of the labels neutral, negative or positive, the text is:\"\n",
    "\n",
    "inputs = tokenizer.encode(sample, HYPOTHESIS, return_tensors=\"pt\")\n",
    "gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "output = model.generate(inputs, gen_config, return_dict_in_generate=True, output_scores=True)[\"scores\"][0]\n",
    "\n",
    "mask = output[0] >= 0\n",
    "indices = mask.nonzero()\n",
    "tokenizer.decode(indices.transpose(0, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23759f0-d799-479b-b89a-b592a6bfb98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.4, 'precision': 0.6, 'f1': 0.4800000000000001}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "from typing import List\n",
    "\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def calculate_metrics(references: List[int], predictions: List[int]):\n",
    "    \"\"\"\n",
    "    Calculate recall, precision and f1 scores.\n",
    "    :returns: Dict of metric name to value across all predictions\n",
    "    \"\"\"\n",
    "    return recall_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys())) | \\\n",
    "        precision_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys())) | \\\n",
    "        f1_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys()))\n",
    "\n",
    "calculate_metrics([1, 1, 1, 2, 0], [1, 1, 0, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee87c7b3-9f7e-4380-8aaa-17c6c10343d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "def classify_with_step_reports(tokenizer: AutoTokenizer, model: AutoModelForCausalLM, sample: str, model_name: str, step: int) -> List[int]:\n",
    "    if VERBOSE:\n",
    "        print(f\"[{model_name}] classification step {step} / {len(target_dataset)}\")\n",
    "    return classify(tokenizer, model, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608b0d5-578e-42d3-804c-1a12dca5e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out different hyper parameters on a single model (Bloom 560m) for the train set.\n",
    "references = target_dataset[\"label\"]\n",
    "predictions = [ classify_with_step_reports(tokenizer, model, sample, \"bloom 560m\", i) for i, sample in enumerate(target_dataset[\"text\"]) ]\n",
    "\n",
    "calculate_metrics(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34db79dc-8682-4428-877c-ab950d3259f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bradlet/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bradlet/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bradlet/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bradlet/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bradlet/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bradlet/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bigscience/bloom-560m',\n",
       "  {'recall': 0.3333333333333333,\n",
       "   'precision': 0.1111111111111111,\n",
       "   'f1': 0.16666666666666666}),\n",
       " ('bigscience/bloom-1b1',\n",
       "  {'recall': 0.3333333333333333,\n",
       "   'precision': 0.1111111111111111,\n",
       "   'f1': 0.16666666666666666}),\n",
       " ('bigscience/bloom-1b7',\n",
       "  {'recall': 0.34942528735632183,\n",
       "   'precision': 0.23391110202133825,\n",
       "   'f1': 0.2677628446522097}),\n",
       " ('bigscience/bloomz-560m',\n",
       "  {'recall': 0.36091954022988504,\n",
       "   'precision': 0.4005203031331297,\n",
       "   'f1': 0.22110362913083095}),\n",
       " ('bigscience/bloomz-1b1',\n",
       "  {'recall': 0.4517241379310345,\n",
       "   'precision': 0.33381423918508313,\n",
       "   'f1': 0.35386716943845653}),\n",
       " ('bigscience/bloomz-1b7',\n",
       "  {'recall': 0.3816091954022989,\n",
       "   'precision': 0.3732510288065844,\n",
       "   'f1': 0.260952380952381})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run across all models \n",
    "from typing import Dict\n",
    "\n",
    "def run(model_name: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Get a tokenizer and model by model name, then classify all of `target_dataset` and calculate metrics.\n",
    "    :returns: Dict of calculated metrics for this model\n",
    "    \"\"\"\n",
    "    if VERBOSE:\n",
    "        print(f\"Starting classification on {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    target_dataset = dataset[\"test\"]\n",
    "    references = target_dataset[\"label\"]\n",
    "    predictions = [ classify_with_step_reports(tokenizer, model, sample, model_name, i) for i, sample in enumerate(target_dataset[\"text\"]) ]\n",
    "    return calculate_metrics(references, predictions)\n",
    "\n",
    "result_metrics = [ (model, run(model)) for model in MODELS ]\n",
    "result_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
