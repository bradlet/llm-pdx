{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2364fc63-683d-44cf-bccd-b29c2adec0fd",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Bradley Thompson - CS 510 Large Language Models PDX Winter 2024\n",
    "\n",
    "## Experimental Setting 1\n",
    "To start, I spent a long time just trying to figure out how to implement the classifier for this assignment. I got started right away, but blasted through many hours just trying to figure it out, not finding any really good resources online. I found one that threw me off base, because I started looking into using huggingface's provided AutoModelClassifiers. After talking to the professor, I learned that this was not the intention for the solution, so I switched back to basing my work on assignment 1. I didn't gain any significant headway until the TA linked some articles in Slack, which gave me a base to research on. I eventually got to the point that I realized how I could check the output scores for all tokens in the tokenizer's vocabulary, and then get the highest log probability for the tokens that mapped to my target labels, to get the models classification.\n",
    "\n",
    "At this point I ran into a few more residual issues around calculating metrics for the multi-class predictions, and also ran into an issue with the vocab because apparently bloom doesn't have \"Neutral\" in its embeddings. I wanted to make my labels case-insensitive with the hope that it would improve accuracy, so I went ahead and made a map to consider both component tokens (\"Neut\" and \"ral\") the same as \"neutral\".\n",
    "\n",
    "Finally, I was able to play around with hyper parameters. The above work quantifies almost all waking hours for a single weekend, and then many multi-hour sessions after work across 2 weeks. Luckily, I have a pretty nice GPU at home, and have CUDA support, so I am able to run the models pretty fast for no added Colab cost.\n",
    "\n",
    "I had no fancy configuration other than setting the model to only generate 1 new token to start. My f1 score was around `0.16` with Bloom 560m on the training set. I set out with the goal of trying to maximize this to some extent, before running my classifier implementation for all models across the test set.\n",
    "\n",
    "First hyperparameter tweak was to try out `top_k` sampling; `top_k=2` had no effect on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce174414-cc4e-40b3-a26a-ef1ca2f4cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "MODELS = (\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"bigscience/bloomz-1b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    ")\n",
    "\n",
    "model_name = MODELS[0]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba1cd0c-c9ca-48e0-ad44-0fc713b4a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"Frank Gaffrey\\\\u002c Cliff May\\\\u002c Steve Emerson: Brilliant. \\\\\"\"\"\"Looming Threats: Iran\\\\u002c Hezbollah Hamas\\\\\"\"\"\" is the best #cufidc session I\\\\u2019ve had thus far.\" ',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\")\n",
    "target_dataset = dataset[\"train\"]\n",
    "\n",
    "# Check out what the data looks like:\n",
    "target_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e56a4f8-3cd6-456f-95bc-e0754d296423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering vocab indices for target tokens: negative Negative neutral Neutral positive Positive\n",
      "Token 111017: negative\n",
      "Token 149414:  Negative\n",
      "Token 40979:  neutral\n",
      "Token 76420:  Neut\n",
      "Token 4343: ral\n",
      "Token 18121:  positive\n",
      "Token 139904:  Positive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[111017, 149414, 40979, 76420, 4343, 18121, 139904]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "POSITIVE=\"positive\"\n",
    "NEUTRAL=\"neutral\"\n",
    "NEGATIVE=\"negative\"\n",
    "\n",
    "\n",
    "# Note: we have 2 options so we can be case insensitive\n",
    "LABELS = \"negative Negative neutral Neutral positive Positive\"\n",
    "ID_TO_LABEL = {\n",
    "    0: NEGATIVE,\n",
    "    1: NEUTRAL,\n",
    "    2: POSITIVE,\n",
    "}\n",
    "# Note: Found that bloom apparently doesn't have `Neutral` in its vocabulary; so, will consider either `Neut` or `ral`.\n",
    "TOKEN_INDEX_TO_LABEL_ID = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 2,\n",
    "    6: 2,\n",
    "}\n",
    "\n",
    "target_label_tokens = tokenizer(LABELS, add_special_tokens=False)[\"input_ids\"]\n",
    "print(\"Gathering vocab indices for target tokens:\", tokenizer.decode(target_label_tokens))\n",
    "for token in target_label_tokens:\n",
    "    print(f\"Token {token}: {tokenizer.decode(token)}\")\n",
    "target_label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38ea228c-026c-45ee-b3c1-9ab45fe6c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: okay i\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\u2019mon America aren\\u2019t you sick of her yet? (sorry) \n",
      "Classification: negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "import torch as t\n",
    "\n",
    "config = {\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 4,\n",
    "}\n",
    "\n",
    "HYPOTHESIS = \"This text has a positive sentiment, true or false:\"\n",
    "\n",
    "def classify(premise: str) -> int:\n",
    "    \"\"\"\n",
    "    Use model to generate output scores across entire vocab for the next token based on probability\n",
    "    of entailment for the given premise/hypothesis pair.\n",
    "    https://joeddav.github.io/blog/2020/05/29/ZSL.html#Classification-as-Natural-Language-Inference\n",
    "    :param premise: some input text string to be classified based on `LABELS`.\n",
    "    :returns: classification label id\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(premise, HYPOTHESIS, return_tensors=\"pt\")\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    # Get prediction scores across vocab for the only token generated b/c of gen config and normalize w/ softmax\n",
    "    output = model.generate(inputs, gen_config, return_dict_in_generate=True, output_scores=True)[\"scores\"][0]\n",
    "    vocab_probs = output.softmax(dim=1)\n",
    "    # Get probabilities of our target labels by index in our vocab\n",
    "    labels_log_probs = t.index_select(vocab_probs, 1, t.tensor(target_label_tokens))[0]\n",
    "    # Get highest log prob label\n",
    "    labels_index = t.argmax(labels_log_probs).item()\n",
    "    return TOKEN_INDEX_TO_LABEL_ID[labels_index]\n",
    "\n",
    "sample = target_dataset[0][\"text\"]\n",
    "print(f\"Input text: {sample}\\nClassification: {ID_TO_LABEL[classify(sample)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a23759f0-d799-479b-b89a-b592a6bfb98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.4}\n",
      "{'precision': 0.6}\n",
      "{'f1': 0.4800000000000001}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "print(recall_metric.compute(references=[1, 1, 1, 2, 0], predictions=[1, 1, 0, 0, 2], average=\"weighted\", labels=list(ID_TO_LABEL.keys())))\n",
    "print(precision_metric.compute(references=[1, 1, 1, 2, 0], predictions=[1, 1, 0, 0, 2], average=\"weighted\", labels=list(ID_TO_LABEL.keys())))\n",
    "print(f1_metric.compute(references=[1, 1, 1, 2, 0], predictions=[1, 1, 0, 0, 2], average=\"weighted\", labels=list(ID_TO_LABEL.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608b0d5-578e-42d3-804c-1a12dca5e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = target_dataset[\"label\"]\n",
    "predictions = [ classify(sample) for sample in target_dataset[\"text\"] ]\n",
    "\n",
    "print(recall_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys())))\n",
    "print(precision_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys())))\n",
    "print(f1_metric.compute(references=references, predictions=predictions, average=\"weighted\", labels=list(ID_TO_LABEL.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
