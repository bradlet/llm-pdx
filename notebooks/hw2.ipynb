{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19305860-7cc3-4088-b5f2-12287a051ad5",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Bradley Thompson - CS 510 LLM Winter 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36b3ff4-9c83-427c-b406-da6d04e26b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PROMPT_TEMPLATE = \"Of the categories 'positive', 'negative' and 'neutral', the statement '%s' is\"\n",
    "GEN_CONFIG = {\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"use_cache\": False,\n",
    "}\n",
    "\n",
    "MODELS = [\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"bigscience/bloomz-1b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b1ab66-bbd9-4e65-a988-6fdeadd0cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from typing import Dict, Any\n",
    "import evaluate\n",
    "import torch as t\n",
    "\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "def gen(model, tokenizer, config, prompt):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    return model.generate(inputs, gen_config)\n",
    "\n",
    "def calculate_metrics(model_name, tokenizer, output):\n",
    "    print(\"Raw: \", output)\n",
    "    input_text = tokenizer.decode(output)\n",
    "    print(\"Decoded: \", input_text)\n",
    "    ttr = len(t.unique(output)) / len(output)\n",
    "    print(\"TTR: \", ttr)\n",
    "    # https://stackoverflow.com/questions/75886674/how-to-compute-sentence-level-perplexity-from-hugging-face-language-models\n",
    "    results = perplexity.compute(model_id=model_name, add_start_token=False, predictions=[input_text])\n",
    "    print(\"Perplexity: \", results[\"perplexities\"][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b53e345-bf80-4002-a774-bfd08eba1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODELS[0]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a25216d-091c-4a3f-b66c-b84e5a5f927e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[96675, 23381, 40979]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "categories: np.array = tokenizer.encode(\"positive negative neutral\", return_tensors=\"pt\").numpy()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26c8d9e3-c872-4179-b6e7-bcdc3b40916d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`num_beams` has to be an integer strictly greater than 1, but is 1. For `num_beams` == 1, one should make use of `greedy_search` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokay i\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124mm sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124mmon America aren\u001b[39m\u001b[38;5;130;01m\\u2019\u001b[39;00m\u001b[38;5;124mt you sick of her yet? (sorry)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m DEFAULT_PROMPT_TEMPLATE \u001b[38;5;241m%\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m calculate_metrics(model_name, tokenizer, outputs[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mgen\u001b[0;34m(model, tokenizer, config, prompt)\u001b[0m\n\u001b[1;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m gen_config: GenerationConfig \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_dict(config)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1684\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m         final_constraints\u001b[38;5;241m.\u001b[39mappend(constraint)\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m-> 1684\u001b[0m constrained_beam_scorer \u001b[38;5;241m=\u001b[39m \u001b[43mConstrainedBeamSearchScorer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_early_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beam_hyps_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# 12. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1696\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1697\u001b[0m     expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1698\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1700\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/beam_search.py:486\u001b[0m, in \u001b[0;36mConstrainedBeamSearchScorer.__init__\u001b[0;34m(self, batch_size, num_beams, constraints, device, length_penalty, do_early_stopping, num_beam_hyps_to_keep, num_beam_groups, max_length)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_beams, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m num_beams \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_beams` has to be an integer strictly greater than 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_beams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For `num_beams` == 1,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m one should make use of `greedy_search` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_beam_groups, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (num_beam_groups \u001b[38;5;241m>\u001b[39m num_beams) \u001b[38;5;129;01mor\u001b[39;00m (num_beams \u001b[38;5;241m%\u001b[39m num_beam_groups \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m divisible by `num_beam_groups`, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_beam_groups\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with `num_beams` being \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_beams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `num_beams` has to be an integer strictly greater than 1, but is 1. For `num_beams` == 1, one should make use of `greedy_search` instead."
     ]
    }
   ],
   "source": [
    "categorization_config = GEN_CONFIG | {\"force_words_ids\": categories.tolist()}\n",
    "\n",
    "input = \"okay i\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\u2019mon America aren\\u2019t you sick of her yet? (sorry)\"\n",
    "prompt = DEFAULT_PROMPT_TEMPLATE % input\n",
    "\n",
    "outputs = gen(model, tokenizer, categorization_config, prompt)\n",
    "calculate_metrics(model_name, tokenizer, outputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
