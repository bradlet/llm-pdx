{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19305860-7cc3-4088-b5f2-12287a051ad5",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "Bradley Thompson - CS 510 LLM Winter 2024\n",
    "\n",
    "## 1\n",
    "Three differences between bloom and bloom z models:\n",
    "1. Bloom seems to be more likely to generate output in first person, versus second or third person for bloomz.\n",
    "2. Bloomz responses seem to be more dire / dark (e.g. \"the world was in a state of war\").\n",
    "3. Bloomz is fine-tuned to a task mixture designed by the researchers.\n",
    "\n",
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b1ab66-bbd9-4e65-a988-6fdeadd0cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from typing import Dict, Any\n",
    "import evaluate\n",
    "import torch as t\n",
    "\n",
    "DEFAULT_PROMPT = \"Once upon a time \"\n",
    "\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "def load_and_gen(model_name: str, prompt: str, config: Dict[str, Any]):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    return tokenizer, model.generate(inputs, gen_config)\n",
    "\n",
    "def calculate_metrics(model_name, tokenizer, output):\n",
    "    print(\"Raw: \", output)\n",
    "    input_text = tokenizer.decode(output)\n",
    "    print(\"Decoded: \", input_text)\n",
    "    ttr = len(t.unique(output)) / len(output)\n",
    "    print(\"TTR: \", ttr)\n",
    "    # https://stackoverflow.com/questions/75886674/how-to-compute-sentence-level-perplexity-from-hugging-face-language-models\n",
    "    results = perplexity.compute(model_id=model_name, add_start_token=False, predictions=[input_text])\n",
    "    print(\"Perplexity: \", results[\"perplexities\"][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c8d9e3-c872-4179-b6e7-bcdc3b40916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([ 64393,  14591,    267,   3509,    210,    473,   1620,    267,   1517,\n",
      "           461,    368,   8876,    189,   5024,    473,   3866,    267,  32531,\n",
      "           530,    267,   2158,    189,   6475,    473,   4984,   1130,    722,\n",
      "           267,  27782,    189,     44,   4984,   3804,    722,    368,   1517,\n",
      "           189,   2175,   1517,   5268,   3866,    427,    722,    189,     36,\n",
      "          1517,   1002,    267,  23724,    189,  11848,    267,  23037,    189,\n",
      "          4670,    722,  11045,    427,  19134,    189,   6895,    861,    473,\n",
      "          9746,    722,  88889,    189,   7594,    368,   6199,    189,  57647,\n",
      "         88889,   1074,    189,  22654,  88889,   2670,  27782,    530,   2670,\n",
      "         27432,    189,  10367,  27782,   1620,    368,   2592,   5268,  88889,\n",
      "           368,   7733,    189,   5563,  88889,   3868,  32531,    189,  21449,\n",
      "         88889,   6371,    189,  55857,  19134,   1620,   1427,  10087,    189,\n",
      "         15466,   7154,   4984,  11705,   1542,  88889,  23588,   4384,    189,\n",
      "          3548,   1620,   3269,    267,  19134,    861,   1620, 167220,    189,\n",
      "         70895,    267,  10440,    861,  11705,  36813,    189,  52637,   7154,\n",
      "         88889,   1152,    189,   5448,   3121,    368,   3804,   2592,    189,\n",
      "         11090,   1728,    368,    854,    189,   1411,    368,  15032,   8876,\n",
      "          5268,   4984,  19134,   1152,   1427,    189,   7171,   1152,   3121,\n",
      "          1427,   7220,    189,  28032,  19134,   7086,   1074,  38782,    189,\n",
      "         15169,    473,  25338,   1152,    361,    368, 230206,    189,   3700,\n",
      "          1152,  37468,    919,   1074,    361,   2670,  32046,    388,  20742,\n",
      "           189,   5715,    368,   3509,   3262,    473,  15776,    861,   1152,\n",
      "          3866,  36813,   1380,     44,   1620,  38782,    530])\n",
      "Decoded:  Once upon a time  I was a man of the world\n",
      "And I had a wife and a son\n",
      "But I could not be a father\n",
      "I could only be the man\n",
      "The man who had to be\n",
      "A man with a heart\n",
      "With a soul\n",
      "To be able to love\n",
      "So that I might be loved\n",
      "By the people\n",
      "Who loved me\n",
      "They loved my father and my mother\n",
      "My father was the one who loved the best\n",
      "He loved his wife\n",
      "She loved him\n",
      "Her love was so great\n",
      "That she could never have loved anyone else\n",
      "It was like a love that was eternal\n",
      "Like a life that never died\n",
      "Because she loved you\n",
      "You were the only one\n",
      "Of all the men\n",
      "In the whole world who could love you so\n",
      "For you were so good\n",
      "Your love made me happy\n",
      "When I saw you in the sunlight\n",
      "As you looked at me in my darkest hours\n",
      "At the time when I thought that you had died,\n",
      "I was happy and\n",
      "TTR:  0.4682926829268293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455c5ef173dc455a839cb04141ece0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  6.80005407333374\n"
     ]
    }
   ],
   "source": [
    "tokenizer_560m, outputs_560m = load_and_gen(\"bigscience/bloom-560m\", DEFAULT_PROMPT, {\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloom-560m\", tokenizer_560m, outputs_560m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d036e0a-ebbd-405f-ba5f-99bdc8fee1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1b1, outputs_1b1 = load_and_gen(\"bigscience/bloom-1b1\", DEFAULT_PROMPT, {\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloom-1b1\", tokenizer_1b1, outputs_1b1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81d32c-bc20-4bcf-9fee-be00c10b95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1b7, outputs_1b7 = load_and_gen(\"bigscience/bloom-1b7\", DEFAULT_PROMPT, {\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"max_new_tokens\": 200,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloom-1b7\", tokenizer_1b7, outputs_1b7[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14f3a0-d414-4956-9d11-d15479fba70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_z560m, outputs_z560m = load_and_gen(\"bigscience/bloomz-560m\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"num_beams\": 2, # Need to use beem sampling or else bloomz stops early\n",
    "    \"early_stopping\": \"never\",\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-560m\", tokenizer_z560m, outputs_z560m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88973e-d0ee-4e55-8900-f9817ded9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_z1b1, outputs_z1b1 = load_and_gen(\"bigscience/bloomz-1b1\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"num_beams\": 2,\n",
    "    \"early_stopping\": \"never\",\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-560m\", tokenizer_z1b1, outputs_z1b1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffd923-a049-43ef-95d7-436630ccacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_z1b7, outputs_z1b7 = load_and_gen(\"bigscience/bloomz-1b7\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"num_beams\": 2,\n",
    "    \"early_stopping\": \"never\",\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-1b7\", tokenizer_z1b7, outputs_z1b7[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07a7b0-552d-4a8a-9583-d04df5c97beb",
   "metadata": {},
   "source": [
    "### 2.a\n",
    "The regular bloom models were easy-enough to interact with, but the bloomz models kept stopping early. I tried a lot of things to get it to stop using the EOS token, but no matter what it generated short output sequences. Eventually I found a way to do it when I found the 'early_stopping' config. When I set that to 'never', I got an error saying I needed to use beam sampling, so I set the beam count to 2 (5 originally but it was way too slow).\n",
    "\n",
    "### 2.b\n",
    "TTR and Perplexity metrics calculated inline in cell outputs above. In general all of the models have very low TTR, which is probably because of the limited textual input to build off of. Perplexity seems to be mildly higher in the bloomz models' outputs; however, this doesn't manifest in a noticeably different or less-repetitive generated sequence.\n",
    "\n",
    "### 2.c\n",
    "The generated stories are all pretty bad. For whatever reason, all models generate extremely repetitive sequences. The bloomz 1.1b parameter model seemed to have the most variety in that it alternated what phrases it output. In general, however, I'd say the creativity, coherence and overall quality of all model outputs were quite low.\n",
    "\n",
    "## 3\n",
    "Increasing model parameters seems to have a positive effect in that it lowers model perplexity. I think it would be better if we provided more context to perhaps see if the models would generate more diverse output. Type-to-token Ration seems relatively unaffected by the parameter size, so perhaps that is a metric that is affected more by the vocabulary for the model's training / pre-training data. Since the bloomz models are built off the bloom models, it makes sense that they have a similar vocabulary and similar TTR results.\n",
    "\n",
    "## 4\n",
    "I think that the two best models were the 560m and 1.1b parameter bloomz models. This is because of the mild improvement in vocublary diversity in the generated sequences. I tried experimenting with temperature, top_k and top_p decoding. It is really hard to get the model to avoid stopping early when not using beam sampling to explicitly tell it not to. I found that top_k sampling didn't have much effect on the repetitive behavior, which makes sense because the model is already clearly stuck on the same highly-probably tokens to choose from. Temperature seems to have a solid effect on diversifying the generated stories; it seems like injecting more randomness helps break out of the repetitive generation pattern. \n",
    "\n",
    "A mix of `temperature` and `top_p` decoding seem to be the most effective hyperparameters to tweak. In the bloomz-1b1 example below, I found that I was able to introduce an adequately lengthed sequence with some level of diversity in the content -- the phrases were not EXACTLY the same, repeated over and over, anymore! I started with a very low temperature of 0.3, then increased it to 0.8. I was a little surprised, thinking that more randomness would result in a greater diversity of content in the generated sequence. Conversely, less randomness from a higher temperature actually allowed the story to build off of itself. The bloomz-1b1 output at the bottom of this document shows a story that, while still not very good and a little repetitive, is actually a story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49242aa6-f686-4a57-ad05-58fe0629b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_z560m, outputs_z560m = load_and_gen(\"bigscience/bloomz-560m\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 10,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-560m\", tokenizer_z560m, outputs_z560m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3180f-92ba-47f3-95bd-bad9a45cf9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_z1b1, outputs_z1b1 = load_and_gen(\"bigscience/bloomz-1b1\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 40,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-560m\", tokenizer_z1b1, outputs_z1b1[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
