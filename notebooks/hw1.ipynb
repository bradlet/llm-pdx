{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19305860-7cc3-4088-b5f2-12287a051ad5",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "Bradley Thompson - CS 510 LLM Winter 2024\n",
    "\n",
    "## 1\n",
    "Three differences between bloom and bloom z models:\n",
    "1. Bloom seems to be more likely to generate output in first person, versus second or third person for bloomz.\n",
    "2. Bloomz responses seem to be more dire / dark (e.g. \"the world was in a state of war\").\n",
    "3. Bloomz is fine-tuned to a task mixture designed by the researchers.\n",
    "\n",
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b1ab66-bbd9-4e65-a988-6fdeadd0cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from typing import Dict, Any\n",
    "import evaluate\n",
    "import torch as t\n",
    "\n",
    "DEFAULT_PROMPT = \"Once upon a time \"\n",
    "\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "def load_and_gen(model_name: str, prompt: str, config: Dict[str, Any]):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    gen_config: GenerationConfig = GenerationConfig.from_dict(config)\n",
    "    return tokenizer, model.generate(inputs, gen_config)\n",
    "\n",
    "def calculate_metrics(model_name, tokenizer, output):\n",
    "    print(\"Raw: \", output)\n",
    "    input_text = tokenizer.decode(output)\n",
    "    print(\"Decoded: \", input_text)\n",
    "    ttr = len(t.unique(output)) / len(output)\n",
    "    print(\"TTR: \", ttr)\n",
    "    # https://stackoverflow.com/questions/75886674/how-to-compute-sentence-level-perplexity-from-hugging-face-language-models\n",
    "    results = perplexity.compute(model_id=model_name, add_start_token=False, predictions=[input_text])\n",
    "    print(\"Perplexity: \", results[\"perplexities\"][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c8d9e3-c872-4179-b6e7-bcdc3b40916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([64393, 14591,   267,  3509,   210,   473,  1620,   267,  1517,   461,\n",
      "          368,  8876,   189,  5024,   473,  1620,   267,  1517,   461,   368,\n",
      "         8876,   189,  5024,   473,  1620,   267,  1517,   461,   368,  8876,\n",
      "          189,  5024,   473,  1620,   267,  1517,   461,   368,  8876,   189,\n",
      "         5024,   473,  1620,   267,  1517,   461,   368,  8876,   189,  5024,\n",
      "          473,  1620,   267,  1517,   461,   368,  8876,   189,  5024,   473,\n",
      "         1620,   267,  1517,   461,   368,  8876,   189,  5024,   473,  1620,\n",
      "          267,  1517,   461,   368,  8876,   189,  5024,   473,  1620,   267,\n",
      "         1517,   461,   368,  8876,   189,  5024,   473,  1620,   267,  1517,\n",
      "          461,   368,  8876,   189,  5024,   473,  1620,   267,  1517,   461,\n",
      "          368,  8876,   189,  5024,   473,  1620,   267,  1517,   461,   368,\n",
      "         8876,   189,  5024,   473,  1620,   267,  1517,   461,   368,  8876,\n",
      "          189,  5024,   473,  1620,   267,  1517,   461,   368,  8876,   189,\n",
      "         5024,   473,  1620,   267,  1517,   461,   368,  8876,   189,  5024,\n",
      "          473,  1620,   267,  1517,   461,   368,  8876,   189,  5024,   473,\n",
      "         1620,   267,  1517,   461,   368,  8876,   189,  5024,   473,  1620,\n",
      "          267,  1517,   461,   368,  8876,   189,  5024,   473,  1620,   267,\n",
      "         1517,   461,   368,  8876,   189,  5024,   473,  1620,   267,  1517,\n",
      "          461,   368,  8876,   189,  5024,   473,  1620,   267,  1517,   461,\n",
      "          368,  8876,   189,  5024,   473,  1620,   267,  1517,   461,   368,\n",
      "         8876,   189,  5024,   473,  1620])\n",
      "Decoded:  Once upon a time  I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was a man of the world\n",
      "And I was\n",
      "TTR:  0.06341463414634146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34b59dd95c44d0fb60dbaed337fa4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1.3040893077850342\n"
     ]
    }
   ],
   "source": [
    "tokenizer_560m, outputs_560m = load_and_gen(\"bigscience/bloom-560m\", DEFAULT_PROMPT, {\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"max_new_tokens\": 200,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloom-560m\", tokenizer_560m, outputs_560m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d036e0a-ebbd-405f-ba5f-99bdc8fee1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([64393, 14591,   267,  3509,   210,   473,  1620,   267, 20500,  1517,\n",
      "          189,    44,  1620,   267, 20500,  1517,   189,    44,  1620,   267,\n",
      "        20500,  1517,   189,    44,  1620,   267, 20500,  1517,   189,    44,\n",
      "         1620,   267, 20500,  1517,   189,    44,  1620,   267, 20500,  1517,\n",
      "          189,    44,  1620,   267, 20500,  1517,   189,    44,  1620,   267,\n",
      "        20500,  1517,   189,    44,  1620,   267, 20500,  1517,   189,    44,\n",
      "         1620,   267, 20500,  1517,   189,    44,  1620,   267, 20500,  1517,\n",
      "          189,    44,  1620,   267, 20500,  1517,   189,    44,  1620,   267,\n",
      "        20500,  1517,   189,    44,  1620,   267, 20500,  1517,   189,    44,\n",
      "         1620,   267, 20500,  1517,   189,    44,  1620,   267, 20500,  1517,\n",
      "          189,    44,  1620,   267, 20500,  1517,   189,    44,  1620,   267,\n",
      "        20500,  1517,   189,    44,  1620,   267, 20500,  1517,   189,    44,\n",
      "         1620,   267, 20500,  1517,   189,    44,  1620,   267, 20500,  1517,\n",
      "          189,    44,  1620,   267, 20500,  1517,   189,    44,  1620,   267,\n",
      "        20500,  1517,   189,    44,  1620,   267, 20500,  1517,   189,    44,\n",
      "         1620,   267, 20500,  1517,   189,    44,  1620,   267, 20500,  1517,\n",
      "          189,    44,  1620,   267, 20500,  1517,   189,    44,  1620,   267,\n",
      "        20500,  1517,   189,    44,  1620,   267, 20500,  1517,   189,    44,\n",
      "         1620,   267, 20500,  1517,   189,    44,  1620,   267, 20500,  1517,\n",
      "          189,    44,  1620,   267, 20500,  1517,   189,    44,  1620,   267,\n",
      "        20500,  1517,   189,    44,  1620])\n",
      "Decoded:  Once upon a time  I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was a young man\n",
      "I was\n",
      "TTR:  0.05365853658536585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a7295ab81c4f0a992305e7628a4511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1.2422466278076172\n"
     ]
    }
   ],
   "source": [
    "tokenizer_1b1, outputs_1b1 = load_and_gen(\"bigscience/bloom-1b1\", DEFAULT_PROMPT, {\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"max_new_tokens\": 200,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloom-1b1\", tokenizer_1b1, outputs_1b1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c81d32c-bc20-4bcf-9fee-be00c10b95d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([64393, 14591,   267,  3509,   210,   473,  1620,   267, 10512, 27566,\n",
      "          189,  5024,   473,  1620,   267, 10512, 27566,   189,    44,  1620,\n",
      "          267, 10512, 27566,   189,    44,  1620,   267, 10512, 27566,   189,\n",
      "           44,  1620,   267, 10512, 27566,   189,    44,  1620,   267, 10512,\n",
      "        27566,   189,    44,  1620,   267, 10512, 27566,   189,    44,  1620,\n",
      "          267, 10512, 27566,   189,    44,  1620,   267, 10512, 27566,   189,\n",
      "           44,  1620,   267, 10512, 27566,   189,    44,  1620,   267, 10512,\n",
      "        27566,   189,    44,  1620,   267, 10512, 27566,   189,    44,  1620,\n",
      "          267, 10512, 27566,   189,    44,  1620,   267, 10512, 27566,   189,\n",
      "           44,  1620,   267, 10512, 27566,   189,    44,  1620,   267, 10512,\n",
      "        27566,   189,    44,  1620,   267, 10512, 27566,   189,    44,  1620,\n",
      "          267, 10512, 27566,   189,    44,  1620,   267, 10512, 27566,   189,\n",
      "           44,  1620,   267, 10512, 27566,   189,    44,  1620,   267, 10512,\n",
      "        27566,   189,    44,  1620,   267, 10512, 27566,   189,    44,  1620,\n",
      "          267, 10512, 27566,   189,    44,  1620,   267, 10512, 27566,   189,\n",
      "           44,  1620,   267, 10512, 27566,   189,    44,  1620,   267, 10512,\n",
      "        27566,   189,    44,  1620,   267, 10512, 27566,   189,    44,  1620,\n",
      "          267, 10512, 27566,   189,    44,  1620,   267, 10512, 27566,   189,\n",
      "           44,  1620,   267, 10512, 27566,   189,    44,  1620,   267, 10512,\n",
      "        27566,   189,    44,  1620,   267, 10512, 27566,   189,    44,  1620,\n",
      "          267, 10512, 27566,   189,    44])\n",
      "Decoded:  Once upon a time  I was a little girl\n",
      "And I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I was a little girl\n",
      "I\n",
      "TTR:  0.05853658536585366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2dd15b5ab34b4db42653df7c08acd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1.259759545326233\n"
     ]
    }
   ],
   "source": [
    "tokenizer_1b7, outputs_1b7 = load_and_gen(\"bigscience/bloom-1b7\", DEFAULT_PROMPT, {\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"max_new_tokens\": 200,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloom-1b7\", tokenizer_1b7, outputs_1b7[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f14f3a0-d414-4956-9d11-d15479fba70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([64393, 14591,   267,  3509,   210,  3262,   368,  8876,  1620,   361,\n",
      "          267,  8431,   461, 27307,    15,   368,  8876,  1620,   361,   267,\n",
      "         8431,   461, 11327,    17,  1387,  8876,  1620,   361,   267,  8431,\n",
      "          461, 11327,    17,  1387,  8876,  1620,   361,   267,  8431,   461,\n",
      "        11327,    17,  1387,  8876,  1620,   361,   267,  8431,   461, 11327,\n",
      "           17,  1387,  8876,  1620,   361,   267,  8431,   461, 11327,    17,\n",
      "         1387,  8876,  1620,   361,   267,  8431,   461, 11327,    17,  1387,\n",
      "         8876,  1620,   361,   267,  8431,   461, 11327,    17,  1387,  8876,\n",
      "         1620,   361,   267,  8431,   461, 11327,    17,  1387,  8876,  1620,\n",
      "          361,   267,  8431,   461, 11327,    17,  1387,  8876,  1620,   361,\n",
      "          267,  8431,   461, 11327,    17,  1387,  8876,  1620,   361,   267,\n",
      "         8431,   461, 11327,    17,  1387,  8876,  1620,   361,   267,  8431,\n",
      "          461, 11327,    17,  1387,  8876,  1620,   361,   267,  8431,   461,\n",
      "        11327,    17,  1387,  8876,  1620,   361,   267,  8431,   461, 11327,\n",
      "           17,  1387,  8876,  1620,   361,   267,  8431,   461, 11327,    17,\n",
      "         1387,  8876,  1620,   361,   267,  8431,   461, 11327,    17,  1387,\n",
      "         8876,  1620,   361,   267,  8431,   461, 11327,    17,  1387,  8876,\n",
      "         1620,   361,   267,  8431,   461, 11327,    17,  1387,  8876,  1620,\n",
      "          361,   267,  8431,   461, 11327,    17,  1387,  8876,  1620,   361,\n",
      "          267,  8431,   461, 11327,    17,  1387,  8876,  1620,   361,   267])\n",
      "Decoded:  Once upon a time  when the world was in a state of peace, the world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a state of war. The world was in a\n",
      "TTR:  0.085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591517d34a054d4aac2847ecfbcdb3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1.5080246925354004\n"
     ]
    }
   ],
   "source": [
    "tokenizer_z560m, outputs_z560m = load_and_gen(\"bigscience/bloomz-560m\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"num_beams\": 2, # Need to use beem sampling or else bloomz stops early\n",
    "    \"early_stopping\": \"never\",\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-560m\", tokenizer_z560m, outputs_z560m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd88973e-d0ee-4e55-8900-f9817ded9d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30c4bc0fa2d4423bb575a35a3741b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385a31c08a9e4cb4beeaf07654df7ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ac2621de63495eb46df99d20758129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8174924c28094456ac2d643fcb3ef97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96562641852407fb755ce01a5ca2e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([ 64393,  14591,    267,   3509,    210,    473,   4853,    427,   2909,\n",
      "           361,    267,  22933,  16333,     17,    473,  31045,    361,    267,\n",
      "        219748,     17,    473,  31045,    361,    267,  22933,  10172,     17,\n",
      "           473,  31045,    361,    267, 219748,     17,    473,  31045,    361,\n",
      "           267,  22933,  10172,     17,    473,  31045,    361,    267,  22933,\n",
      "         10172,     17,    473,  31045,    361,    267,  22933,  10172,     17,\n",
      "           473,  31045,    361,    267,  22933,  10172,     17,    473,  31045,\n",
      "           361,    267,  22933,  10172,     17,    473,  31045,    361,    267,\n",
      "         22933,  10172,     17,    473,  31045,    361,    267,  22933,  10172,\n",
      "            17,    473,  31045,    361,    267,  22933,  10172,     17,    473,\n",
      "         31045,    361,    267,  22933,  10172,     17,    473,  31045,    361,\n",
      "           267,  22933,  10172,     17,    473,  31045,    361,    267,  22933,\n",
      "         10172,     17,    473,  31045,    361,    267,  22933,  10172,     17,\n",
      "           473,  31045,    361,    267,  22933,  10172,     17,    473,  31045,\n",
      "           361,    267,  22933,  10172,     17,    473,  31045,    361,    267,\n",
      "         22933,  10172,     17,    473,  31045,    361,    267,  22933,  10172,\n",
      "            17,    473,  31045,    361,    267,  22933,  10172,     17,    473,\n",
      "         31045,    361,    267,  22933,  10172,     17,    473,  31045,    361,\n",
      "           267,  22933,  10172,     17,    473,  31045,    361,    267,  22933,\n",
      "         10172,     17,    473,  31045,    361,    267,  22933,  10172,     17,\n",
      "           473,  31045,    361,    267,  22933,  10172,     17,    473,  31045,\n",
      "           361,    267,  22933,  10172,     17,    473,  31045,    361,    267,\n",
      "         22933,  10172])\n",
      "Decoded:  Once upon a time  I used to work in a construction company. I worked in a warehouse. I worked in a construction site. I worked in a warehouse. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site. I worked in a construction site\n",
      "TTR:  0.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a124398b168140aeac40086cdd138915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1.580476999282837\n"
     ]
    }
   ],
   "source": [
    "tokenizer_z1b1, outputs_z1b1 = load_and_gen(\"bigscience/bloomz-1b1\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"num_beams\": 2,\n",
    "    \"early_stopping\": \"never\",\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-560m\", tokenizer_z1b1, outputs_z1b1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ffd923-a049-43ef-95d7-436630ccacb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e65e4074db4a9eb9754b30a9f764dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([64393, 14591,   267,  3509,   210,  2782,  1620,   267, 21380, 25922,\n",
      "         8749,   210,  5268,  1620,   267,  7220, 21380,   210,  5298,  1620,\n",
      "          267,  7220, 21380,   210,  5298,  1620,   267,  7220, 21380,   210,\n",
      "         5298,  1620,   267,  7220, 21380,   210,  5298,  1620,   267,  7220,\n",
      "        21380,   210,  5298,  1620,   267,  7220, 21380,   210,  5298,  1620,\n",
      "          267,  7220, 21380,   210,  5298,  1620,   267,  7220, 21380,   210,\n",
      "         5298,  1620,   267,  7220, 21380,   210,  5298,  1620,   267,  7220,\n",
      "        21380,   210,  5298,  1620,   267,  7220, 21380,   210,  5298,  1620,\n",
      "          267,  7220, 21380,   210,  5298,  1620,   267,  7220, 21380,   210,\n",
      "         5298,  1620,   267,  7220, 21380,   210,  5298,  1620,   267,  7220,\n",
      "        21380,   210,  5298,  1620,   267,  7220, 21380,   210,  5298,  1620,\n",
      "          267,  7220, 21380,   210,  5298,  1620,   267,  7220, 21380,   210,\n",
      "         5298,  1620,   267,  7220, 21380,   210,  5298,  1620,   267,  7220,\n",
      "        21380,   210,  5298,  1620,   267,  7220, 21380,   210,  5298,  1620,\n",
      "          267,  7220, 21380,   210,  5298,  1620,   267,  7220, 21380,   210,\n",
      "         5298,  1620,   267,  7220, 21380,   210,  5298,  1620,   267,  7220,\n",
      "        21380,   210,  5298,  1620,   267,  7220, 21380,   210,  5298,  1620,\n",
      "          267,  7220, 21380,   210,  5298,  1620,   267,  7220, 21380,   210,\n",
      "         5298,  1620,   267,  7220, 21380,   210,  5298,  1620,   267,  7220,\n",
      "        21380,   210,  5298,  1620,   267,  7220, 21380,   210,  5298,  1620])\n",
      "Decoded:  Once upon a time  there was a boy named John  who was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was a good boy  He was\n",
      "TTR:  0.065\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e024b8650a4b0da62679a313532d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1.368320107460022\n"
     ]
    }
   ],
   "source": [
    "tokenizer_z1b7, outputs_z1b7 = load_and_gen(\"bigscience/bloomz-1b7\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"num_beams\": 2,\n",
    "    \"early_stopping\": \"never\",\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-1b7\", tokenizer_z1b7, outputs_z1b7[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07a7b0-552d-4a8a-9583-d04df5c97beb",
   "metadata": {},
   "source": [
    "### 2.a\n",
    "The regular bloom models were easy-enough to interact with, but the bloomz models kept stopping early. I tried a lot of things to get it to stop using the EOS token, but no matter what it generated short output sequences. Eventually I found a way to do it when I found the 'early_stopping' config. When I set that to 'never', I got an error saying I needed to use beam sampling, so I set the beam count to 2 (5 originally but it was way too slow).\n",
    "\n",
    "### 2.b\n",
    "TTR and Perplexity metrics calculated inline in cell outputs above. In general all of the models have very low TTR, which is probably because of the limited textual input to build off of. Perplexity seems to be mildly higher in the bloomz models' outputs; however, this doesn't manifest in a noticeably different or less-repetitive generated sequence.\n",
    "\n",
    "### 2.c\n",
    "The generated stories are all pretty bad. For whatever reason, all models generate extremely repetitive sequences. The bloomz 1.1b parameter model seemed to have the most variety in that it alternated what phrases it output. In general, however, I'd say the creativity, coherence and overall quality of all model outputs were quite low.\n",
    "\n",
    "## 3\n",
    "Increasing model parameters seems to have a positive effect in that it lowers model perplexity. I think it would be better if we provided more context to perhaps see if the models would generate more diverse output. Type-to-token Ration seems relatively unaffected by the parameter size, so perhaps that is a metric that is affected more by the vocabulary for the model's training / pre-training data. Since the bloomz models are built off the bloom models, it makes sense that they have a similar vocabulary and similar TTR results.\n",
    "\n",
    "## 4\n",
    "I think that the two best models were the 560m and 1.1b parameter bloomz models. This is because of the mild improvement in vocublary diversity in the generated sequences. I tried experimenting with temperature, top_k and top_p decoding. It is really hard to get the model to avoid stopping early when not using beam sampling to explicitly tell it not to. I found that top_k sampling didn't have much effect on the repetitive behavior, which makes sense because the model is already clearly stuck on the same highly-probably tokens to choose from. Temperature seems to have a solid effect on diversifying the generated stories; it seems like injecting more randomness helps break out of the repetitive generation pattern. \n",
    "\n",
    "A mix of `temperature` and `top_p` decoding seem to be the most effective hyperparameters to tweak. In the bloomz-1b1 example below, I found that I was able to introduce an adequately lengthed sequence with some level of diversity in the content -- the phrases were not EXACTLY the same, repeated over and over, anymore! I started with a very low temperature of 0.3, then increased it to 0.8. I was a little surprised, thinking that more randomness would result in a greater diversity of content in the generated sequence. Conversely, less randomness from a higher temperature actually allowed the story to build off of itself. The bloomz-1b1 output at the bottom of this document shows a story that, while still not very good and a little repetitive, is actually a story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49242aa6-f686-4a57-ad05-58fe0629b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([64393, 14591,   267,  3509,   210,   368, 22779, 11705, 83315,   368,\n",
      "        60312,    17,  5298,  1620,   361,   368, 42723, 12978,     2])\n",
      "Decoded:  Once upon a time  the sun never reaches the horizon. He was in the outer space</s>\n",
      "TTR:  0.8947368421052632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c01c62336e94f2c911a66c3d7c92d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  39.0067024230957\n"
     ]
    }
   ],
   "source": [
    "tokenizer_z560m, outputs_z560m = load_and_gen(\"bigscience/bloomz-560m\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 10,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-560m\", tokenizer_z560m, outputs_z560m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc3180f-92ba-47f3-95bd-bad9a45cf9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:  tensor([ 64393,  14591,    267,   3509,    210,    368,  29880,   2623,   3276,\n",
      "         29497,   6486,    368,  40634,    530,  14005,    919,    267,  37426,\n",
      "          1865,  15114,   3276,  16299,  82406,    267,  25170,    461,  57498,\n",
      "           919,    368,  37426,   1865,  14831,  65095,     15,   7154,  57247,\n",
      "           267, 170804,    461,  57498,     17,   1387,  29880,   2623,   1256,\n",
      "         27432,  57247,   8512,  25170,    461,  57498,     15,    530,    267,\n",
      "         15325,  25170,   1620,   9507,   7160,   1865,   1387,  29880,   2623,\n",
      "         35081,    427,   6482,   7154,   1620,   7963, 153034,     17,   2685,\n",
      "         23997,    661,    368,  29880,   2623,  22726,  82859,     15,    368,\n",
      "         29880,   2623,  35081,    427,  16184,    267, 162443,    361,   3809,\n",
      "        149630,     17,  15114,   8610,   1427, 206861,    427,  14005,    861,\n",
      "          7154,  29763,  41097,    267, 136507,     17,   1387,  29880,   2623,\n",
      "         35081,    427,  47010,    530,  35081,    427,  16184,    267,  57383,\n",
      "           461,  63524,    291,     17,   3162,   1620,    368,  29880,   2623,\n",
      "          5268,  35081,    427,  63524,    291,   1865,   1387,  29880,   2623,\n",
      "         35081,    427,  86159,    336,   2175,  29880,   2623,   1256,  27432,\n",
      "         13716,    427,  77205,   3809,   6486,     15,   1965,   1620, 211264,\n",
      "            17,   1387,  29880,   2623,   8610,   1427, 153034,    861,   7154,\n",
      "          4984,   1130,  14005,  82859,     17,      2])\n",
      "Decoded:  Once upon a time  the narrator would walk down the street and stop at a restaurant.  She would often purchase a plate of soup at the restaurant.  One evening, she ordered a bowl of soup. The narrator's mother ordered another plate of soup, and a third plate was left without.  The narrator began to think she was going hungry. As soon as the narrator started eating, the narrator began to feel a lump in her throat. She got so desperate to stop that she grabbed a fork. The narrator began to eat and began to feel a rush of vomit. It was the narrator who began to vomit.  The narrator began to cry.\n",
      "The narrator's mother tried to calm her down, but was unsuccessful. The narrator got so hungry that she could not stop eating.</s>\n",
      "TTR:  0.4523809523809524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e49841612d04b8097a2b047d91b760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  9.218196868896484\n"
     ]
    }
   ],
   "source": [
    "tokenizer_z1b1, outputs_z1b1 = load_and_gen(\"bigscience/bloomz-1b1\", DEFAULT_PROMPT, {\n",
    "    \"min_length\": 100,\n",
    "    \"max_length\": 200,\n",
    "    \"use_cache\": False,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 40,\n",
    "})\n",
    "calculate_metrics(\"bigscience/bloomz-560m\", tokenizer_z1b1, outputs_z1b1[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
